{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install kornia\n",
    "# ! pip install -U albumentations[imgaug]\n",
    "# ! pip install neptune-client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment training dataset build by using \n",
    " 2000 Synthetic images from [Impaintin](https://www.grip.unina.it/download/vipcup2022/gated_convolution_inpainting.zip) dataset | places in the folder names \"Generated\" \\\n",
    " and 5000 real images from [COCO2017](http://images.cocodataset.org/zips/val2017.zip) dataset | placed in the folder name \"Real\"\n",
    "\n",
    "data set folder structure\n",
    "```\n",
    "|SampleData |- label.csv\n",
    "            |- Generated /\n",
    "            |- Real /\n",
    "```\n",
    "\n",
    "__Download & save the backborne model before any experiment run ; \\\n",
    "xception model : http://data.lip6.fr/cadene/pretrainedmodels/xception-43020ad28.pth \\\n",
    "and save in the 'C:\\Users\\deela\\.cache\\torch\\hub\\checkpoints' directory__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import json \n",
    "import logging\n",
    "import warnings\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.multiprocessing as mp\n",
    "import torch.nn.functional as F\n",
    "from torch.cuda.amp import autocast\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.distributed as dist\n",
    "from models.MAT import MAT\n",
    "# from datasets.dataset import DeepfakeDataset\n",
    "from AGDA import AGDA\n",
    "import cv2\n",
    "from utils import dist_average,ACC\n",
    "from config import train_config\n",
    "import neptune.new as neptune\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from datasets.augmentations import augmentations\n",
    "from albumentations import CenterCrop,Compose,Resize,RandomCrop\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from torch.utils.tensorboard import SummaryWriter\n",
    "cv2.setNumThreads(0)\n",
    "cv2.ocl.setUseOpenCL(False)\n",
    "# GPU settings\n",
    "assert torch.cuda.is_available()\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "#torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://app.neptune.ai/Botz/VIPCup-logs/e/VIP-4\n",
      "Remember to stop your run once youâ€™ve finished logging your metadata (https://docs.neptune.ai/api-reference/run#.stop). It will be stopped automatically only when the notebook kernel/interactive console is terminated.\n"
     ]
    }
   ],
   "source": [
    "run = neptune.init(\n",
    "    project=\"Botz/VIPCup-logs\",\n",
    "    api_token=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiJkNWJjMDdhNC05NWY5LTQwNWQtYTQyNi0zNjNmYmYwZDg3M2YifQ==\",\n",
    ")  # your credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepfakeDataset(Dataset):\n",
    "    def __init__(self, df, resize=(320,320), augment='augment0', normalize=dict(mean=[0.5,0.5,0.5],std=[0.5,0.5,0.5]), phase='train'):\n",
    "        self.df = df \n",
    "        self.aug=augmentations[augment]\n",
    "        self.file_paths = self.df['file_path'].values \n",
    "        self.labels = self.df['label'].values \n",
    "        self.trans=Compose([RandomCrop(*resize),ToTensorV2()])\n",
    "        self.phase = phase\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.file_paths[idx]\n",
    "        image_label = self.labels[idx]\n",
    "        image = cv2.imread(image_path, 1)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "        # image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "        # orig_image = torch.from_numpy(np.transpose(image, (2,0,1)))\n",
    "        # resized_image = Resize(size=self.image_size)(orig_image)\n",
    "#         return {'image': resized_image, 'label': torch.from_numpy(image_label)}\n",
    "        image=self.aug(image=image)['image']\n",
    "        final_image = self.trans(image=image)['image']\n",
    "        return final_image, torch.from_numpy(np.array(image_label))\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.df.shape[0]\n",
    "\n",
    "    def next_epoch(self):\n",
    "        self.epoch+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_path = './data/SampleData/labels.csv'\n",
    "ann_df = pd.read_csv(ann_path)\n",
    "X, y = ann_df.iloc[:, :-1], ann_df.iloc[:, -1]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>imageId</th>\n",
       "      <th>file_path</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5516</th>\n",
       "      <td>000000407518</td>\n",
       "      <td>./data/SampleData\\Real\\000000407518.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3266</th>\n",
       "      <td>000000149568</td>\n",
       "      <td>./data/SampleData\\Real\\000000149568.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6468</th>\n",
       "      <td>000000517832</td>\n",
       "      <td>./data/SampleData\\Real\\000000517832.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4752</th>\n",
       "      <td>000000319100</td>\n",
       "      <td>./data/SampleData\\Real\\000000319100.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2958</th>\n",
       "      <td>000000113403</td>\n",
       "      <td>./data/SampleData\\Real\\000000113403.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           imageId                                file_path  label\n",
       "5516  000000407518  ./data/SampleData\\Real\\000000407518.jpg      0\n",
       "3266  000000149568  ./data/SampleData\\Real\\000000149568.jpg      0\n",
       "6468  000000517832  ./data/SampleData\\Real\\000000517832.jpg      0\n",
       "4752  000000319100  ./data/SampleData\\Real\\000000319100.jpg      0\n",
       "2958  000000113403  ./data/SampleData\\Real\\000000113403.jpg      0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = X_train.copy()\n",
    "train_df['label'] = y_train\n",
    "\n",
    "eval_df = X_test.copy()\n",
    "eval_df['label'] = y_test\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_state(net,ckpt):\n",
    "    sd=net.state_dict()\n",
    "    nd={}\n",
    "    goodmatch=True\n",
    "    for i in ckpt:\n",
    "        if i in sd and sd[i].shape==ckpt[i].shape:\n",
    "            nd[i]=ckpt[i]\n",
    "            #print(i)\n",
    "        else:\n",
    "            print('fail to load %s'%i)\n",
    "            goodmatch=False\n",
    "    net.load_state_dict(nd,strict=False)\n",
    "    return goodmatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loss(loss_pack,config):\n",
    "    if 'loss' in loss_pack:\n",
    "        return loss_pack['loss']\n",
    "    loss=config.ensemble_loss_weight*loss_pack['ensemble_loss']+config.aux_loss_weight*loss_pack['aux_loss']\n",
    "    if config.AGDA_loss_weight!=0:\n",
    "        loss+=config.AGDA_loss_weight*loss_pack['AGDA_ensemble_loss']+config.match_loss_weight*loss_pack['match_loss']\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_step(logs,data_loader,net,optimizer,device,config,AG=None,phase='train'):\n",
    "    if config.AGDA_loss_weight==0:\n",
    "        AG=None\n",
    "    recorder={}\n",
    "    if config.feature_layer=='logits':\n",
    "        record_list=['loss','acc']\n",
    "    else:\n",
    "        record_list=['ensemble_loss','aux_loss','ensemble_acc']\n",
    "        if AG is not None:\n",
    "            record_list+=['AGDA_ensemble_loss','match_loss']\n",
    "    for i in record_list:\n",
    "        recorder[i]=dist_average(device)\n",
    "    # begin training\n",
    "    start_time = time.time()\n",
    "    if phase=='train':\n",
    "        net.train()\n",
    "    else: net.eval()\n",
    "    with tqdm(data_loader, unit=\"batch\") as tepoch:\n",
    "        for X, y in tepoch:\n",
    "            X = X.float().to(device)\n",
    "            y = y.to(device)\n",
    "            with torch.set_grad_enabled(phase=='train'):\n",
    "                with autocast():\n",
    "                    loss_pack=net(X,y,train_batch=True,AG=AG)\n",
    "            if phase=='train':\n",
    "                batch_loss = train_loss(loss_pack,config)\n",
    "                batch_loss.backward()\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "            with torch.no_grad():\n",
    "                if config.feature_layer=='logits':\n",
    "                    loss_pack['acc']=ACC(loss_pack['logits'],y)\n",
    "                else:\n",
    "                    loss_pack['ensemble_acc']=ACC(loss_pack['ensemble_logit'],y)\n",
    "            for i in record_list:\n",
    "                recorder[i].step(loss_pack[i])\n",
    "            tepoch.set_postfix({r : recorder[r].get() for r in record_list})\n",
    "                \n",
    "\n",
    "    # end of this epoch\n",
    "    batch_info=[]\n",
    "    for i in record_list:\n",
    "        mesg=recorder[i].get()\n",
    "        logs[i]=mesg\n",
    "        batch_info.append('{}:{:.4f}'.format(i,mesg))\n",
    "    end_time = time.time()\n",
    "\n",
    "    # write log for this epoch\n",
    "    if phase == 'train':\n",
    "        for i in record_list:\n",
    "            run[f'train/{i}'].log(recorder[i].get())\n",
    "    else:\n",
    "        for i in record_list:\n",
    "            run[f'eval/{i}'].log(recorder[i].get())\n",
    "    logging.info('{}: {}, Time {:3.2f}'.format(phase,'  '.join(batch_info), end_time - start_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_worker(config):\n",
    "    # rank=local_rank+rank_offset\n",
    "    logging.basicConfig(\n",
    "    filename=os.path.join('runs', config.name,'train.log'),\n",
    "    filemode='a',\n",
    "    format='%(asctime)s: %(levelname)s: [%(filename)s:%(lineno)d]: %(message)s',\n",
    "    level=logging.INFO)\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    # dist.init_process_group(backend='nccl', init_method=config.url,world_size=world_size, rank=rank)\n",
    "    # if rank==0:\n",
    "    #     try:\n",
    "    #         os.remove('/tmp/.pytorch_distribute')\n",
    "    #     except:\n",
    "    #         pass\n",
    "    np.random.seed(1234567)\n",
    "    torch.manual_seed(1234567)\n",
    "    torch.cuda.manual_seed(1234567)\n",
    "    # torch.cuda.set_device(local_rank)\n",
    "\n",
    "    print(\"Start Data preparation ...\")\n",
    "    train_dataset = DeepfakeDataset(phase='train',**config.train_dataset)\n",
    "    validate_dataset=DeepfakeDataset(phase='test',**config.val_dataset)\n",
    "    # train_sampler=torch.utils.data.distributed.DistributedSampler(train_dataset)\n",
    "    # validate_sampler=torch.utils.data.distributed.DistributedSampler(validate_dataset)\n",
    "    train_loader=torch.utils.data.DataLoader(train_dataset, batch_size=config.batch_size, pin_memory=True)\n",
    "    validate_loader=torch.utils.data.DataLoader(validate_dataset, batch_size=config.batch_size, pin_memory=True)\n",
    "    print(\"Successfully complete Data preparation ...\")\n",
    "    logs = {}\n",
    "    start_epoch = 0\n",
    "    net = MAT(**config.net_config)\n",
    "    for i in config.freeze:\n",
    "        if 'backbone' in i:\n",
    "            net.net.requires_grad_(False)\n",
    "        elif 'attention' in i:\n",
    "            net.attentions.requires_grad_(False)\n",
    "        elif 'feature_center' in i:\n",
    "            net.auxiliary_loss.alpha=0\n",
    "        elif 'texture_enhance' in i:\n",
    "            net.texture_enhance.requires_grad_(False)\n",
    "        elif 'fcs' in i:\n",
    "            net.projection_local.requires_grad_(False)\n",
    "            net.project_final.requires_grad_(False)\n",
    "            net.ensemble_classifier_fc.requires_grad_(False)\n",
    "        else:\n",
    "            if 'xception' in str(type(net.net)):\n",
    "                for j in net.net.seq:\n",
    "                    if j[0]==i:\n",
    "                        for t in j[1]:\n",
    "                            t.requires_grad_(False)\n",
    "            \n",
    "            if 'EfficientNet' in str(type(net.net)):\n",
    "                if i=='b0':\n",
    "                    net.net._conv_stem.requires_grad_(False)\n",
    "                stage_map=net.net.stage_map\n",
    "                for c in range(len(stage_map)-2,-1,-1):\n",
    "                    if not stage_map[c]:\n",
    "                        stage_map[c]=stage_map[c+1]\n",
    "                for c1,c2 in zip(stage_map,net.net._blocks):\n",
    "                    if c1==i:\n",
    "                        c2.requires_grad_(False)\n",
    "    print(\"Model Initialzation ...\")\n",
    "    # net=nn.SyncBatchNorm.convert_sync_batchnorm(net)#.to(local_rank)\n",
    "    net.to(DEVICE)\n",
    "    AG=AGDA(**config.AGDA_config)#.to(local_rank)\n",
    "    optimizer = torch.optim.AdamW(net.parameters(), lr=config.learning_rate, betas=config.adam_betas, weight_decay=config.weight_decay)\n",
    "    scheduler=torch.optim.lr_scheduler.StepLR(optimizer, step_size=config.scheduler_step, gamma=config.scheduler_gamma)\n",
    "    if config.ckpt:\n",
    "        loc = DEVICE\n",
    "        checkpoint = torch.load(config.ckpt, map_location=loc)\n",
    "        logs = checkpoint['logs']\n",
    "        start_epoch = int(logs['epoch'])+1\n",
    "        if load_state(net.module,checkpoint['state_dict']) and config.resume_optim:\n",
    "            optimizer.load_state_dict(checkpoint['optimizer_state'])\n",
    "            try:\n",
    "                scheduler.load_state_dict(checkpoint['scheduler_state'])\n",
    "            except:\n",
    "                pass\n",
    "        else:\n",
    "            net.module.auxiliary_loss.alpha=torch.tensor(config.alpha)\n",
    "        del checkpoint\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"Start Model Training ...\")\n",
    "    for epoch in range(start_epoch, config.epochs):\n",
    "        print(f'[EPOCH] - {epoch}')\n",
    "        logs['epoch'] = epoch\n",
    "        # train_sampler.set_epoch(epoch)\n",
    "        # train_sampler.dataset.next_epoch()\n",
    "        run_step(logs=logs, data_loader=train_loader, net=net, optimizer=optimizer, device=DEVICE, config=config, AG=AG, phase='train')\n",
    "        run_step(logs=logs, data_loader=validate_loader, net=net, optimizer=optimizer, device=DEVICE, config=config, phase='valid')\n",
    "        net.auxiliary_loss.alpha*=config.alpha_decay # because of 'module' not found error.\n",
    "        scheduler.step()\n",
    "    else :\n",
    "        torch.save({\n",
    "                'logs': logs,\n",
    "                'state_dict': net.state_dict(),\n",
    "                'optimizer_state': optimizer.state_dict(),\n",
    "                'scheduler_state':scheduler.state_dict()}, 'checkpoints/'+config.name+'/ckpt_%s.pth'%epoch)\n",
    "        # dist.barrier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distributed_train(config,world_size=0,num_gpus=0,rank_offset=0):\n",
    "    if not num_gpus:\n",
    "        num_gpus = torch.cuda.device_count()\n",
    "    if not world_size:\n",
    "        world_size=num_gpus\n",
    "    mp.spawn(main_worker, nprocs=num_gpus, args=(world_size,rank_offset,config))\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"Model-Name\": 'multi-attention model',\n",
    "    'Dataset-Name': 'Custome-testingV2',\n",
    "    'note': 'experiment with RGB color format input images',\n",
    "    \"attention_layer\": \"b5\",\n",
    "    \"batch_size\": 16,\n",
    "    \"Augmentations\": \"horiz. flip, random crop\",\n",
    "    \"learning_rate\": 0.001, \n",
    "    \"optimizer\": \"AdamW\",\n",
    "}\n",
    "run[\"parameters\"] = params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Data preparation ...\n",
      "Successfully complete Data preparation ...\n",
      "Model Initialzation ...\n",
      "Start Model Training ...\n",
      "[EPOCH] - 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 88/88 [02:06<00:00,  1.43s/batch, loss=0.433, acc=0.8]  \n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 88/88 [00:25<00:00,  3.42batch/s, loss=0.182, acc=0.95] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EPOCH] - 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 88/88 [01:07<00:00,  1.29batch/s, loss=0.154, acc=0.952]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 88/88 [00:24<00:00,  3.59batch/s, loss=0.0626, acc=0.99] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EPOCH] - 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 88/88 [01:07<00:00,  1.31batch/s, loss=0.06, acc=0.989]  \n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 88/88 [00:23<00:00,  3.68batch/s, loss=0.0253, acc=0.997]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EPOCH] - 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 88/88 [01:07<00:00,  1.30batch/s, loss=0.0402, acc=0.989]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 88/88 [00:24<00:00,  3.66batch/s, loss=0.0188, acc=0.995]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EPOCH] - 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 88/88 [01:07<00:00,  1.31batch/s, loss=0.0292, acc=0.991]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 88/88 [00:23<00:00,  3.67batch/s, loss=0.0171, acc=0.997]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shutting down background jobs, please wait a moment...\n",
      "Done!\n",
      "Waiting for the remaining 12 operations to synchronize with Neptune. Do not kill this process.\n",
      "All 12 operations synced, thanks for waiting!\n",
      "Explore the metadata in the Neptune app:\n",
      "https://app.neptune.ai/Botz/VIPCup-logs/e/VIP-4\n"
     ]
    }
   ],
   "source": [
    "name='rgb_test'\n",
    "url='tcp://127.0.0.1:27015'\n",
    "Config=train_config(name,['custom'],url=url, df=train_df, eval_df=eval_df, attention_layer='b5',feature_layer='logits',epochs=5,batch_size=16,AGDA_loss_weight=0, augment='augment0')\n",
    "Config.mkdirs()\n",
    "main_worker(Config) \n",
    "run.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Model loading and inferencing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'net': 'xception',\n",
       " 'feature_layer': 'logits',\n",
       " 'attention_layer': 'b5',\n",
       " 'num_classes': 2,\n",
       " 'M': 4,\n",
       " 'mid_dims': 256,\n",
       " 'dropout_rate': 0.25,\n",
       " 'drop_final_rate': 0.5,\n",
       " 'pretrained': '',\n",
       " 'alpha': 0.05,\n",
       " 'size': (200, 200),\n",
       " 'margin': 0.5,\n",
       " 'inner_margin': [0.1, -2]}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Config.net_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open('./tmp/model_config.json', 'w') as pf:\n",
    "    json.dump(Config.net_config, pf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(chk_path, config_path):\n",
    "    with open(config_path, 'r') as pf:\n",
    "        model_config = json.load(pf)\n",
    "\n",
    "    model = MAT(**model_config)\n",
    "    model_checkpoint = torch.load(chk_path)\n",
    "    model.load_state_dict(model_checkpoint['state_dict'])\n",
    "    model.eval()\n",
    "\n",
    "    return model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model = load_model('./checkpoints/Efb4/ckpt_4.pth', './tmp/model_config.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MAT(\n",
       "  (net): xception(\n",
       "    (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
       "    (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu1): ReLU(inplace=True)\n",
       "    (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "    (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu2): ReLU(inplace=True)\n",
       "    (block1): Block(\n",
       "      (skip): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "      (skipbn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (rep): Sequential(\n",
       "        (0): SeparableConv2d(\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n",
       "          (pointwise): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): SeparableConv2d(\n",
       "          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n",
       "          (pointwise): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "        (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "      )\n",
       "    )\n",
       "    (block2): Block(\n",
       "      (skip): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "      (skipbn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (rep): Sequential(\n",
       "        (0): ReLU()\n",
       "        (1): SeparableConv2d(\n",
       "          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n",
       "          (pointwise): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "        (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (3): ReLU(inplace=True)\n",
       "        (4): SeparableConv2d(\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n",
       "          (pointwise): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "        (5): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (6): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "      )\n",
       "    )\n",
       "    (block3): Block(\n",
       "      (skip): Conv2d(256, 728, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "      (skipbn): BatchNorm2d(728, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (rep): Sequential(\n",
       "        (0): ReLU()\n",
       "        (1): SeparableConv2d(\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n",
       "          (pointwise): Conv2d(256, 728, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "        (2): BatchNorm2d(728, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (3): ReLU(inplace=True)\n",
       "        (4): SeparableConv2d(\n",
       "          (conv1): Conv2d(728, 728, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=728, bias=False)\n",
       "          (pointwise): Conv2d(728, 728, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "        (5): BatchNorm2d(728, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (6): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "      )\n",
       "    )\n",
       "    (block4): Block(\n",
       "      (rep): Sequential(\n",
       "        (0): ReLU()\n",
       "        (1): SeparableConv2d(\n",
       "          (conv1): Conv2d(728, 728, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=728, bias=False)\n",
       "          (pointwise): Conv2d(728, 728, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "        (2): BatchNorm2d(728, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (3): ReLU(inplace=True)\n",
       "        (4): SeparableConv2d(\n",
       "          (conv1): Conv2d(728, 728, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=728, bias=False)\n",
       "          (pointwise): Conv2d(728, 728, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "        (5): BatchNorm2d(728, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (6): ReLU(inplace=True)\n",
       "        (7): SeparableConv2d(\n",
       "          (conv1): Conv2d(728, 728, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=728, bias=False)\n",
       "          (pointwise): Conv2d(728, 728, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "        (8): BatchNorm2d(728, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (block5): Block(\n",
       "      (rep): Sequential(\n",
       "        (0): ReLU()\n",
       "        (1): SeparableConv2d(\n",
       "          (conv1): Conv2d(728, 728, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=728, bias=False)\n",
       "          (pointwise): Conv2d(728, 728, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "        (2): BatchNorm2d(728, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (3): ReLU(inplace=True)\n",
       "        (4): SeparableConv2d(\n",
       "          (conv1): Conv2d(728, 728, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=728, bias=False)\n",
       "          (pointwise): Conv2d(728, 728, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "        (5): BatchNorm2d(728, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (6): ReLU(inplace=True)\n",
       "        (7): SeparableConv2d(\n",
       "          (conv1): Conv2d(728, 728, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=728, bias=False)\n",
       "          (pointwise): Conv2d(728, 728, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "        (8): BatchNorm2d(728, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (block6): Block(\n",
       "      (rep): Sequential(\n",
       "        (0): ReLU()\n",
       "        (1): SeparableConv2d(\n",
       "          (conv1): Conv2d(728, 728, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=728, bias=False)\n",
       "          (pointwise): Conv2d(728, 728, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "        (2): BatchNorm2d(728, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (3): ReLU(inplace=True)\n",
       "        (4): SeparableConv2d(\n",
       "          (conv1): Conv2d(728, 728, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=728, bias=False)\n",
       "          (pointwise): Conv2d(728, 728, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "        (5): BatchNorm2d(728, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (6): ReLU(inplace=True)\n",
       "        (7): SeparableConv2d(\n",
       "          (conv1): Conv2d(728, 728, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=728, bias=False)\n",
       "          (pointwise): Conv2d(728, 728, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "        (8): BatchNorm2d(728, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (block7): Block(\n",
       "      (rep): Sequential(\n",
       "        (0): ReLU()\n",
       "        (1): SeparableConv2d(\n",
       "          (conv1): Conv2d(728, 728, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=728, bias=False)\n",
       "          (pointwise): Conv2d(728, 728, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "        (2): BatchNorm2d(728, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (3): ReLU(inplace=True)\n",
       "        (4): SeparableConv2d(\n",
       "          (conv1): Conv2d(728, 728, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=728, bias=False)\n",
       "          (pointwise): Conv2d(728, 728, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "        (5): BatchNorm2d(728, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (6): ReLU(inplace=True)\n",
       "        (7): SeparableConv2d(\n",
       "          (conv1): Conv2d(728, 728, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=728, bias=False)\n",
       "          (pointwise): Conv2d(728, 728, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "        (8): BatchNorm2d(728, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (block8): Block(\n",
       "      (rep): Sequential(\n",
       "        (0): ReLU()\n",
       "        (1): SeparableConv2d(\n",
       "          (conv1): Conv2d(728, 728, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=728, bias=False)\n",
       "          (pointwise): Conv2d(728, 728, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "        (2): BatchNorm2d(728, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (3): ReLU(inplace=True)\n",
       "        (4): SeparableConv2d(\n",
       "          (conv1): Conv2d(728, 728, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=728, bias=False)\n",
       "          (pointwise): Conv2d(728, 728, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "        (5): BatchNorm2d(728, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (6): ReLU(inplace=True)\n",
       "        (7): SeparableConv2d(\n",
       "          (conv1): Conv2d(728, 728, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=728, bias=False)\n",
       "          (pointwise): Conv2d(728, 728, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "        (8): BatchNorm2d(728, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (block9): Block(\n",
       "      (rep): Sequential(\n",
       "        (0): ReLU()\n",
       "        (1): SeparableConv2d(\n",
       "          (conv1): Conv2d(728, 728, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=728, bias=False)\n",
       "          (pointwise): Conv2d(728, 728, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "        (2): BatchNorm2d(728, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (3): ReLU(inplace=True)\n",
       "        (4): SeparableConv2d(\n",
       "          (conv1): Conv2d(728, 728, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=728, bias=False)\n",
       "          (pointwise): Conv2d(728, 728, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "        (5): BatchNorm2d(728, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (6): ReLU(inplace=True)\n",
       "        (7): SeparableConv2d(\n",
       "          (conv1): Conv2d(728, 728, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=728, bias=False)\n",
       "          (pointwise): Conv2d(728, 728, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "        (8): BatchNorm2d(728, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (block10): Block(\n",
       "      (rep): Sequential(\n",
       "        (0): ReLU()\n",
       "        (1): SeparableConv2d(\n",
       "          (conv1): Conv2d(728, 728, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=728, bias=False)\n",
       "          (pointwise): Conv2d(728, 728, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "        (2): BatchNorm2d(728, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (3): ReLU(inplace=True)\n",
       "        (4): SeparableConv2d(\n",
       "          (conv1): Conv2d(728, 728, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=728, bias=False)\n",
       "          (pointwise): Conv2d(728, 728, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "        (5): BatchNorm2d(728, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (6): ReLU(inplace=True)\n",
       "        (7): SeparableConv2d(\n",
       "          (conv1): Conv2d(728, 728, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=728, bias=False)\n",
       "          (pointwise): Conv2d(728, 728, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "        (8): BatchNorm2d(728, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (block11): Block(\n",
       "      (rep): Sequential(\n",
       "        (0): ReLU()\n",
       "        (1): SeparableConv2d(\n",
       "          (conv1): Conv2d(728, 728, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=728, bias=False)\n",
       "          (pointwise): Conv2d(728, 728, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "        (2): BatchNorm2d(728, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (3): ReLU(inplace=True)\n",
       "        (4): SeparableConv2d(\n",
       "          (conv1): Conv2d(728, 728, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=728, bias=False)\n",
       "          (pointwise): Conv2d(728, 728, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "        (5): BatchNorm2d(728, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (6): ReLU(inplace=True)\n",
       "        (7): SeparableConv2d(\n",
       "          (conv1): Conv2d(728, 728, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=728, bias=False)\n",
       "          (pointwise): Conv2d(728, 728, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "        (8): BatchNorm2d(728, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (block12): Block(\n",
       "      (skip): Conv2d(728, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "      (skipbn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (rep): Sequential(\n",
       "        (0): ReLU()\n",
       "        (1): SeparableConv2d(\n",
       "          (conv1): Conv2d(728, 728, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=728, bias=False)\n",
       "          (pointwise): Conv2d(728, 728, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "        (2): BatchNorm2d(728, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (3): ReLU(inplace=True)\n",
       "        (4): SeparableConv2d(\n",
       "          (conv1): Conv2d(728, 728, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=728, bias=False)\n",
       "          (pointwise): Conv2d(728, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "        (5): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (6): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "      )\n",
       "    )\n",
       "    (conv3): SeparableConv2d(\n",
       "      (conv1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024, bias=False)\n",
       "      (pointwise): Conv2d(1024, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    )\n",
       "    (bn3): BatchNorm2d(1536, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu3): ReLU(inplace=True)\n",
       "    (conv4): SeparableConv2d(\n",
       "      (conv1): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536, bias=False)\n",
       "      (pointwise): Conv2d(1536, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    )\n",
       "    (bn4): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu4): ReLU(inplace=True)\n",
       "    (last_linear): Linear(in_features=2048, out_features=2, bias=True)\n",
       "  )\n",
       "  (attentions): AttentionMap(\n",
       "    (conv_extract): Conv2d(728, 728, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (bn1): BatchNorm2d(728, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (conv2): Conv2d(728, 4, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (bn2): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (atp): AttentionPooling()\n",
       "  (texture_enhance): Texture_Enhance_v2(\n",
       "    (conv_extract): Conv2d(2, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (conv0): Conv2d(8, 8, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=4)\n",
       "    (conv1): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=4)\n",
       "    (bn1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (conv2): Conv2d(16, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=4)\n",
       "    (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (conv3): Conv2d(24, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=4)\n",
       "    (bn3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (conv_last): Conv2d(32, 8, kernel_size=(1, 1), stride=(1, 1), groups=4)\n",
       "    (bn4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (bn_last): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (projection_local): Sequential(\n",
       "    (0): Linear(in_features=8, out_features=256, bias=True)\n",
       "    (1): Hardswish()\n",
       "    (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "  )\n",
       "  (project_final): Linear(in_features=2048, out_features=256, bias=True)\n",
       "  (ensemble_classifier_fc): Sequential(\n",
       "    (0): Linear(in_features=512, out_features=256, bias=True)\n",
       "    (1): Hardswish()\n",
       "    (2): Linear(in_features=256, out_features=2, bias=True)\n",
       "  )\n",
       "  (auxiliary_loss): Auxiliary_Loss_v2(\n",
       "    (atp): AttentionPooling()\n",
       "  )\n",
       "  (dropout): Dropout2d(p=0.25, inplace=True)\n",
       "  (dropout_final): Dropout(p=0.5, inplace=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_sample(image_path):\n",
    "    image = cv2.imread(image_path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "    print(image.shape)\n",
    "    image = cv2.resize(image, (200, 200))\n",
    "    print(image.shape)\n",
    "    image = np.transpose(image, (2,0,1))[np.newaxis, ...]\n",
    "    print(image.shape)\n",
    "    image_tensor = torch.from_numpy(image).float()\n",
    "    print(image_tensor.shape)\n",
    "    return image_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1250, 1000, 3)\n",
      "(200, 200, 3)\n",
      "(1, 3, 200, 200)\n",
      "torch.Size([1, 3, 200, 200])\n"
     ]
    }
   ],
   "source": [
    "sample_tensor = load_sample('./data/SampleData/1.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_output = torch.softmax(test_model(sample_tensor), dim=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[9.9989e-01, 1.1377e-04]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('vip22')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "81223763d27432f38d2cf1faa13ae6499d96bcdbbc8c12df160497c1881d2009"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
