{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BNuqny-j_eE5",
        "outputId": "272b12d6-5314-4644-8be3-2a1311afeba9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1377GxX9ZupjbUgYTjTEzzqPFsapAHNg_\n",
            "To: /content/major.zip\n",
            "100% 10.1G/10.1G [00:50<00:00, 199MB/s]\n"
          ]
        }
      ],
      "source": [
        "!gdown https://drive.google.com/uc?id=1377GxX9ZupjbUgYTjTEzzqPFsapAHNg_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qDHoGZ33UK_k",
        "outputId": "947563b3-7842-434d-e5f0-2c2b3535d423"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-08-22 15:36:10--  http://data.lip6.fr/cadene/pretrainedmodels/xception-b5690688.pth\n",
            "Resolving data.lip6.fr (data.lip6.fr)... 132.227.201.10\n",
            "Connecting to data.lip6.fr (data.lip6.fr)|132.227.201.10|:80... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://data.lip6.fr/cadene/pretrainedmodels/xception-b5690688.pth [following]\n",
            "--2022-08-22 15:36:10--  https://data.lip6.fr/cadene/pretrainedmodels/xception-b5690688.pth\n",
            "Connecting to data.lip6.fr (data.lip6.fr)|132.227.201.10|:443... connected.\n",
            "WARNING: cannot verify data.lip6.fr's certificate, issued by ‘CN=TERENA SSL CA 3,O=TERENA,L=Amsterdam,ST=Noord-Holland,C=NL’:\n",
            "  Issued certificate has expired.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 91674713 (87M) [application/octet-stream]\n",
            "Saving to: ‘xception-b5690688.pth’\n",
            "\n",
            "xception-b5690688.p 100%[===================>]  87.43M   363KB/s    in 4m 17s  \n",
            "\n",
            "2022-08-22 15:40:28 (348 KB/s) - ‘xception-b5690688.pth’ saved [91674713/91674713]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget --no-check-certificate data.lip6.fr/cadene/pretrainedmodels/xception-b5690688.pth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pMthTH22Utew"
      },
      "outputs": [],
      "source": [
        "!mkdir checkpoints\n",
        "!mkdir checkpoints/F3Net\n",
        "!mv xception-b5690688.pth checkpoints/F3Net"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vbxCFuBhAQ31"
      },
      "outputs": [],
      "source": [
        "!unzip -q major.zip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm major.zip"
      ],
      "metadata": {
        "id": "CGrcWlWPXyB2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install neptune-client torchmetrics"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mSkh9TE-RugD",
        "outputId": "f1e1d842-b0fe-4875-cc80-78f72572a809"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting neptune-client\n",
            "  Downloading neptune-client-0.16.5.tar.gz (320 kB)\n",
            "\u001b[K     |████████████████████████████████| 320 kB 5.0 MB/s \n",
            "\u001b[?25hCollecting torchmetrics\n",
            "  Downloading torchmetrics-0.9.3-py3-none-any.whl (419 kB)\n",
            "\u001b[K     |████████████████████████████████| 419 kB 53.2 MB/s \n",
            "\u001b[?25hCollecting bravado\n",
            "  Downloading bravado-11.0.3-py2.py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.7/dist-packages (from neptune-client) (7.1.2)\n",
            "Collecting future>=0.17.1\n",
            "  Downloading future-0.18.2.tar.gz (829 kB)\n",
            "\u001b[K     |████████████████████████████████| 829 kB 45.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: oauthlib>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from neptune-client) (3.2.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from neptune-client) (1.3.5)\n",
            "Requirement already satisfied: Pillow>=1.1.6 in /usr/local/lib/python3.7/dist-packages (from neptune-client) (7.1.2)\n",
            "Collecting PyJWT\n",
            "  Downloading PyJWT-2.4.0-py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: requests>=2.20.0 in /usr/local/lib/python3.7/dist-packages (from neptune-client) (2.23.0)\n",
            "Requirement already satisfied: requests-oauthlib>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from neptune-client) (1.3.1)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from neptune-client) (1.15.0)\n",
            "Collecting websocket-client!=1.0.0,>=0.35.0\n",
            "  Downloading websocket_client-1.3.3-py3-none-any.whl (54 kB)\n",
            "\u001b[K     |████████████████████████████████| 54 kB 3.7 MB/s \n",
            "\u001b[?25hCollecting GitPython>=2.0.8\n",
            "  Downloading GitPython-3.1.27-py3-none-any.whl (181 kB)\n",
            "\u001b[K     |████████████████████████████████| 181 kB 71.9 MB/s \n",
            "\u001b[?25hCollecting boto3>=1.16.0\n",
            "  Downloading boto3-1.24.56-py3-none-any.whl (132 kB)\n",
            "\u001b[K     |████████████████████████████████| 132 kB 65.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from neptune-client) (21.3)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from neptune-client) (1.24.3)\n",
            "Collecting swagger-spec-validator>=2.7.4\n",
            "  Downloading swagger_spec_validator-2.7.4-py2.py3-none-any.whl (27 kB)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (from neptune-client) (5.4.8)\n",
            "Collecting jsonschema<4.0.0\n",
            "  Downloading jsonschema-3.2.0-py2.py3-none-any.whl (56 kB)\n",
            "\u001b[K     |████████████████████████████████| 56 kB 5.8 MB/s \n",
            "\u001b[?25hCollecting botocore<1.28.0,>=1.27.56\n",
            "  Downloading botocore-1.27.56-py3-none-any.whl (9.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 9.1 MB 56.5 MB/s \n",
            "\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Collecting s3transfer<0.7.0,>=0.6.0\n",
            "  Downloading s3transfer-0.6.0-py3-none-any.whl (79 kB)\n",
            "\u001b[K     |████████████████████████████████| 79 kB 9.5 MB/s \n",
            "\u001b[?25hCollecting urllib3\n",
            "  Downloading urllib3-1.26.12-py2.py3-none-any.whl (140 kB)\n",
            "\u001b[K     |████████████████████████████████| 140 kB 73.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.28.0,>=1.27.56->boto3>=1.16.0->neptune-client) (2.8.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from GitPython>=2.0.8->neptune-client) (4.1.1)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.9-py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 2.3 MB/s \n",
            "\u001b[?25hCollecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from jsonschema<4.0.0->neptune-client) (4.12.0)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema<4.0.0->neptune-client) (22.1.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from jsonschema<4.0.0->neptune-client) (57.4.0)\n",
            "Requirement already satisfied: pyrsistent>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema<4.0.0->neptune-client) (0.18.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20.0->neptune-client) (2.10)\n",
            "Collecting urllib3\n",
            "  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
            "\u001b[K     |████████████████████████████████| 127 kB 70.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20.0->neptune-client) (2022.6.15)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20.0->neptune-client) (3.0.4)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from swagger-spec-validator>=2.7.4->neptune-client) (6.0)\n",
            "Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (1.21.6)\n",
            "Requirement already satisfied: torch>=1.3.1 in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (1.12.1+cu113)\n",
            "Requirement already satisfied: msgpack in /usr/local/lib/python3.7/dist-packages (from bravado->neptune-client) (1.0.4)\n",
            "Collecting bravado-core>=5.16.1\n",
            "  Downloading bravado_core-5.17.0-py2.py3-none-any.whl (67 kB)\n",
            "\u001b[K     |████████████████████████████████| 67 kB 7.3 MB/s \n",
            "\u001b[?25hCollecting monotonic\n",
            "  Downloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
            "Collecting simplejson\n",
            "  Downloading simplejson-3.17.6-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (130 kB)\n",
            "\u001b[K     |████████████████████████████████| 130 kB 74.7 MB/s \n",
            "\u001b[?25hCollecting jsonref\n",
            "  Downloading jsonref-0.2-py3-none-any.whl (9.3 kB)\n",
            "Requirement already satisfied: jsonschema[format]>=2.5.1 in /usr/local/lib/python3.7/dist-packages (from bravado-core>=5.16.1->bravado->neptune-client) (4.3.3)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from bravado-core>=5.16.1->bravado->neptune-client) (2022.2.1)\n",
            "Collecting jsonschema[format]>=2.5.1\n",
            "  Downloading jsonschema-4.14.0-py3-none-any.whl (82 kB)\n",
            "\u001b[K     |████████████████████████████████| 82 kB 813 kB/s \n",
            "\u001b[?25h  Downloading jsonschema-4.13.0-py3-none-any.whl (81 kB)\n",
            "\u001b[K     |████████████████████████████████| 81 kB 9.6 MB/s \n",
            "\u001b[?25h  Downloading jsonschema-4.12.1-py3-none-any.whl (81 kB)\n",
            "\u001b[K     |████████████████████████████████| 81 kB 12.4 MB/s \n",
            "\u001b[?25h  Downloading jsonschema-4.12.0-py3-none-any.whl (81 kB)\n",
            "\u001b[K     |████████████████████████████████| 81 kB 12.2 MB/s \n",
            "\u001b[?25h  Downloading jsonschema-4.11.0-py3-none-any.whl (80 kB)\n",
            "\u001b[K     |████████████████████████████████| 80 kB 10.8 MB/s \n",
            "\u001b[?25h  Downloading jsonschema-4.10.3-py3-none-any.whl (81 kB)\n",
            "\u001b[K     |████████████████████████████████| 81 kB 12.2 MB/s \n",
            "\u001b[?25h  Downloading jsonschema-4.10.2-py3-none-any.whl (81 kB)\n",
            "\u001b[K     |████████████████████████████████| 81 kB 12.0 MB/s \n",
            "\u001b[?25h  Downloading jsonschema-4.10.1-py3-none-any.whl (81 kB)\n",
            "\u001b[K     |████████████████████████████████| 81 kB 11.7 MB/s \n",
            "\u001b[?25h  Downloading jsonschema-4.10.0-py3-none-any.whl (80 kB)\n",
            "\u001b[K     |████████████████████████████████| 80 kB 11.6 MB/s \n",
            "\u001b[?25h  Downloading jsonschema-4.9.1-py3-none-any.whl (79 kB)\n",
            "\u001b[K     |████████████████████████████████| 79 kB 10.2 MB/s \n",
            "\u001b[?25h  Downloading jsonschema-4.9.0-py3-none-any.whl (79 kB)\n",
            "\u001b[K     |████████████████████████████████| 79 kB 10.6 MB/s \n",
            "\u001b[?25h  Downloading jsonschema-4.8.0-py3-none-any.whl (81 kB)\n",
            "\u001b[K     |████████████████████████████████| 81 kB 12.0 MB/s \n",
            "\u001b[?25h  Downloading jsonschema-4.7.2-py3-none-any.whl (81 kB)\n",
            "\u001b[K     |████████████████████████████████| 81 kB 12.2 MB/s \n",
            "\u001b[?25h  Downloading jsonschema-4.7.1-py3-none-any.whl (80 kB)\n",
            "\u001b[K     |████████████████████████████████| 80 kB 9.1 MB/s \n",
            "\u001b[?25h  Downloading jsonschema-4.7.0-py3-none-any.whl (80 kB)\n",
            "\u001b[K     |████████████████████████████████| 80 kB 11.7 MB/s \n",
            "\u001b[?25h  Downloading jsonschema-4.6.2-py3-none-any.whl (80 kB)\n",
            "\u001b[K     |████████████████████████████████| 80 kB 12.6 MB/s \n",
            "\u001b[?25h  Downloading jsonschema-4.6.1-py3-none-any.whl (80 kB)\n",
            "\u001b[K     |████████████████████████████████| 80 kB 12.4 MB/s \n",
            "\u001b[?25h  Downloading jsonschema-4.6.0-py3-none-any.whl (80 kB)\n",
            "\u001b[K     |████████████████████████████████| 80 kB 11.2 MB/s \n",
            "\u001b[?25h  Downloading jsonschema-4.5.1-py3-none-any.whl (72 kB)\n",
            "\u001b[K     |████████████████████████████████| 72 kB 1.9 MB/s \n",
            "\u001b[?25h  Downloading jsonschema-4.5.0-py3-none-any.whl (73 kB)\n",
            "\u001b[K     |████████████████████████████████| 73 kB 1.9 MB/s \n",
            "\u001b[?25h  Downloading jsonschema-4.4.0-py3-none-any.whl (72 kB)\n",
            "\u001b[K     |████████████████████████████████| 72 kB 1.5 MB/s \n",
            "\u001b[?25h  Downloading jsonschema-4.3.2-py3-none-any.whl (71 kB)\n",
            "\u001b[K     |████████████████████████████████| 71 kB 243 kB/s \n",
            "\u001b[?25h  Downloading jsonschema-4.3.1-py3-none-any.whl (71 kB)\n",
            "\u001b[K     |████████████████████████████████| 71 kB 11.3 MB/s \n",
            "\u001b[?25h  Downloading jsonschema-4.3.0-py3-none-any.whl (71 kB)\n",
            "\u001b[K     |████████████████████████████████| 71 kB 11.1 MB/s \n",
            "\u001b[?25h  Downloading jsonschema-4.2.1-py3-none-any.whl (69 kB)\n",
            "\u001b[K     |████████████████████████████████| 69 kB 9.4 MB/s \n",
            "\u001b[?25h  Downloading jsonschema-4.2.0-py3-none-any.whl (69 kB)\n",
            "\u001b[K     |████████████████████████████████| 69 kB 9.4 MB/s \n",
            "\u001b[?25h  Downloading jsonschema-4.1.2-py3-none-any.whl (69 kB)\n",
            "\u001b[K     |████████████████████████████████| 69 kB 9.2 MB/s \n",
            "\u001b[?25h  Downloading jsonschema-4.1.1-py3-none-any.whl (69 kB)\n",
            "\u001b[K     |████████████████████████████████| 69 kB 9.3 MB/s \n",
            "\u001b[?25h  Downloading jsonschema-4.1.0-py3-none-any.whl (69 kB)\n",
            "\u001b[K     |████████████████████████████████| 69 kB 9.2 MB/s \n",
            "\u001b[?25h  Downloading jsonschema-4.0.1-py3-none-any.whl (69 kB)\n",
            "\u001b[K     |████████████████████████████████| 69 kB 9.3 MB/s \n",
            "\u001b[?25hCollecting jsonpointer>1.13\n",
            "  Downloading jsonpointer-2.3-py2.py3-none-any.whl (7.8 kB)\n",
            "Collecting strict-rfc3339\n",
            "  Downloading strict-rfc3339-0.7.tar.gz (17 kB)\n",
            "Collecting webcolors\n",
            "  Downloading webcolors-1.12-py3-none-any.whl (9.9 kB)\n",
            "Collecting rfc3987\n",
            "  Downloading rfc3987-1.3.8-py2.py3-none-any.whl (13 kB)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->jsonschema<4.0.0->neptune-client) (3.8.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->neptune-client) (3.0.9)\n",
            "Building wheels for collected packages: neptune-client, future, strict-rfc3339\n",
            "  Building wheel for neptune-client (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for neptune-client: filename=neptune_client-0.16.5-py2.py3-none-any.whl size=573755 sha256=5a8bf34f3c946d1d69105fc08bf60a3d627d3ca6e6f20cbb059d21902971a913\n",
            "  Stored in directory: /root/.cache/pip/wheels/85/9c/d5/619cf33de6be035e9fcdb8e9c208ce6f45b7b23285468f1d09\n",
            "  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for future: filename=future-0.18.2-py3-none-any.whl size=491070 sha256=751774c79022362b3cc4ff562bba0261199c2482500e3e159e17d3f71363ee2e\n",
            "  Stored in directory: /root/.cache/pip/wheels/56/b0/fe/4410d17b32f1f0c3cf54cdfb2bc04d7b4b8f4ae377e2229ba0\n",
            "  Building wheel for strict-rfc3339 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for strict-rfc3339: filename=strict_rfc3339-0.7-py3-none-any.whl size=18149 sha256=658796405a9232cd8c9ddc148d8403c8c10c9d5273d3ec9470222a5d1ecd4a0a\n",
            "  Stored in directory: /root/.cache/pip/wheels/f3/1d/9f/2a74caecb81b8beb9a4fbe1754203d4b7cf42ef5d39e0d2311\n",
            "Successfully built neptune-client future strict-rfc3339\n",
            "Installing collected packages: webcolors, urllib3, strict-rfc3339, rfc3987, jsonschema, jsonpointer, jmespath, swagger-spec-validator, smmap, simplejson, jsonref, botocore, s3transfer, monotonic, gitdb, bravado-core, websocket-client, PyJWT, GitPython, future, bravado, boto3, torchmetrics, neptune-client\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "  Attempting uninstall: jsonschema\n",
            "    Found existing installation: jsonschema 4.3.3\n",
            "    Uninstalling jsonschema-4.3.3:\n",
            "      Successfully uninstalled jsonschema-4.3.3\n",
            "  Attempting uninstall: future\n",
            "    Found existing installation: future 0.16.0\n",
            "    Uninstalling future-0.16.0:\n",
            "      Successfully uninstalled future-0.16.0\n",
            "Successfully installed GitPython-3.1.27 PyJWT-2.4.0 boto3-1.24.56 botocore-1.27.56 bravado-11.0.3 bravado-core-5.17.0 future-0.18.2 gitdb-4.0.9 jmespath-1.0.1 jsonpointer-2.3 jsonref-0.2 jsonschema-3.2.0 monotonic-1.6 neptune-client-0.16.5 rfc3987-1.3.8 s3transfer-0.6.0 simplejson-3.17.6 smmap-5.0.0 strict-rfc3339-0.7 swagger-spec-validator-2.7.4 torchmetrics-0.9.3 urllib3-1.25.11 webcolors-1.12 websocket-client-1.3.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s10nEKjYEp2W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5bd442c7-753d-4789-eb94-f3f59ebfe692"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "https://app.neptune.ai/Botz/VIPCup-logs/e/VIP-32\n",
            "Remember to stop your run once you’ve finished logging your metadata (https://docs.neptune.ai/api-reference/run#.stop). It will be stopped automatically only when the notebook kernel/interactive console is terminated.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
        "osenvs = len(os.environ['CUDA_VISIBLE_DEVICES'].split(','))\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.utils.model_zoo as model_zoo\n",
        "from torchvision import transforms as trans\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchmetrics import Recall, Precision, F1Score, Accuracy\n",
        "\n",
        "import copy\n",
        "\n",
        "import albumentations as A\n",
        "from albumentations.pytorch.transforms import ToTensorV2\n",
        "from albumentations import RandomCrop\n",
        "\n",
        "from sklearn.metrics import accuracy_score, roc_curve\n",
        "from sklearn.metrics import auc as cal_auc\n",
        "\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "import logging\n",
        "import sys\n",
        "\n",
        "import pandas as pd\n",
        "import cv2 as cv\n",
        "import matplotlib.pyplot as plt \n",
        "\n",
        "import neptune.new as neptune\n",
        "run = neptune.init(\n",
        "    project=\"Botz/VIPCup-logs\",\n",
        "    api_token=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiJkNWJjMDdhNC05NWY5LTQwNWQtYTQyNi0zNjNmYmYwZDg3M2YifQ==\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4jkdvPQeFrm4"
      },
      "outputs": [],
      "source": [
        "with open(\"Major/train.txt\",\"r+\") as f0:\n",
        "  train_list = f0.read().split(\"\\n\")\n",
        "\n",
        "with open(\"Major/val.txt\",\"r+\") as f0:\n",
        "  val_list = f0.read().split(\"\\n\")\n",
        "\n",
        "with open(\"Major/test.txt\",\"r+\") as f0:\n",
        "  test_list = f0.read().split(\"\\n\")\n",
        "\n",
        "labels = pd.read_csv(\"Major/labels.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kkMuFB1YFueA"
      },
      "outputs": [],
      "source": [
        "def construct_dataloder(dataset, batch_size, shuffle=True, num_workers=4):\n",
        "    return DataLoader(dataset, batch_size=batch_size,\n",
        "                      shuffle=shuffle, num_workers=num_workers)\n",
        "\n",
        "class VIPDataset(Dataset):\n",
        "    def __init__(self, file_ids, labels, data_dir,):\n",
        "        self.file_ids = file_ids\n",
        "        self.labels = labels\n",
        "        self.data_dir = data_dir\n",
        "        self.aug = transform = A.Compose([\n",
        "            A.augmentations.geometric.resize.RandomScale(scale_limit=[0,2],interpolation=cv.INTER_LINEAR,p=0.3),\n",
        "            A.augmentations.transforms.ImageCompression(quality_lower=99, quality_upper=100, always_apply=False, p=0.3),\n",
        "            # A.augmentations.crops.transforms.RandomCrop(width=200, height=200,p=1)\n",
        "        ])\n",
        "        self.trans = A.Compose([RandomCrop(width=200, height=200,p=1), ToTensorV2()])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.file_ids)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        image_id = self.file_ids[index]\n",
        "        x = np.array(Image.open(os.path.join(self.data_dir, image_id)))\n",
        "        x = self.aug(image=x)['image']\n",
        "        x = self.trans(image=x)['image']\n",
        "        y = int(self.labels.loc[self.labels[\"image_ids\"] == image_id,\"label\"])\n",
        "        x = 2*x.float()/255.0 - 1\n",
        "        return x, torch.from_numpy(np.array(y))\n",
        "\n",
        "    def augment(self, x):\n",
        "        \"\"\"Augmentations for images\"\"\"\n",
        "        return self.aug_pipeline(image=x)[\"image\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O0x0ZI8yFx6n"
      },
      "outputs": [],
      "source": [
        "train_ds = VIPDataset(train_list,labels,\"Major/data\")\n",
        "val_ds = VIPDataset(val_list,labels,\"Major/data\")\n",
        "test_ds = VIPDataset(test_list,labels,\"Major/data\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CCXxVl1JF1jH"
      },
      "outputs": [],
      "source": [
        "batch_size = 8\n",
        "\n",
        "train_dl = construct_dataloder(train_ds, batch_size, shuffle=True, num_workers=2)\n",
        "val_dl = construct_dataloder(val_ds, batch_size, shuffle=True, num_workers=2)\n",
        "test_dl = construct_dataloder(test_ds, batch_size, shuffle=True, num_workers=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "orse860dB5vt"
      },
      "outputs": [],
      "source": [
        "class SeparableConv2d(nn.Module):\n",
        "    def __init__(self,in_channels,out_channels,kernel_size=1,stride=1,padding=0,dilation=1,bias=False):\n",
        "        super(SeparableConv2d,self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(in_channels,in_channels,kernel_size,stride,padding,dilation,groups=in_channels,bias=bias)\n",
        "        self.pointwise = nn.Conv2d(in_channels,out_channels,1,1,0,1,1,bias=bias)\n",
        "\n",
        "    def forward(self,x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.pointwise(x)\n",
        "        return x\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self,in_filters,out_filters,reps,strides=1,start_with_relu=True,grow_first=True):\n",
        "        super(Block, self).__init__()\n",
        "\n",
        "        if out_filters != in_filters or strides!=1:\n",
        "            self.skip = nn.Conv2d(in_filters,out_filters,1,stride=strides, bias=False)\n",
        "            self.skipbn = nn.BatchNorm2d(out_filters)\n",
        "        else:\n",
        "            self.skip=None\n",
        "\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        rep=[]\n",
        "\n",
        "        filters=in_filters\n",
        "        if grow_first:\n",
        "            rep.append(self.relu)\n",
        "            rep.append(SeparableConv2d(in_filters,out_filters,3,stride=1,padding=1,bias=False))\n",
        "            rep.append(nn.BatchNorm2d(out_filters))\n",
        "            filters = out_filters\n",
        "\n",
        "        for i in range(reps-1):\n",
        "            rep.append(self.relu)\n",
        "            rep.append(SeparableConv2d(filters,filters,3,stride=1,padding=1,bias=False))\n",
        "            rep.append(nn.BatchNorm2d(filters))\n",
        "\n",
        "        if not grow_first:\n",
        "            rep.append(self.relu)\n",
        "            rep.append(SeparableConv2d(in_filters,out_filters,3,stride=1,padding=1,bias=False))\n",
        "            rep.append(nn.BatchNorm2d(out_filters))\n",
        "\n",
        "        if not start_with_relu:\n",
        "            rep = rep[1:]\n",
        "        else:\n",
        "            rep[0] = nn.ReLU(inplace=False)\n",
        "\n",
        "        if strides != 1:\n",
        "            rep.append(nn.MaxPool2d(3,strides,1))\n",
        "        self.rep = nn.Sequential(*rep)\n",
        "\n",
        "    def forward(self,inp):\n",
        "        x = self.rep(inp)\n",
        "\n",
        "        if self.skip is not None:\n",
        "            skip = self.skip(inp)\n",
        "            skip = self.skipbn(skip)\n",
        "        else:\n",
        "            skip = inp\n",
        "\n",
        "        x+=skip\n",
        "        return x\n",
        "\n",
        "# MISSING 'ClassBlock <- classify the final result'\n",
        "\n",
        "class Xception(nn.Module):\n",
        "    \"\"\"\n",
        "    Xception optimized for the ImageNet dataset, as specified in\n",
        "    https://arxiv.org/pdf/1610.02357.pdf\n",
        "    \"\"\"\n",
        "    def __init__(self, num_classes=1000):\n",
        "        \"\"\" Constructor\n",
        "        Args:\n",
        "            num_classes: number of classes\n",
        "        \"\"\"\n",
        "        super(Xception, self).__init__()\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 32, 3,2, 0, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(32)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "        self.conv2 = nn.Conv2d(32,64,3,bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(64)\n",
        "        #do relu here\n",
        "\n",
        "        self.block1=Block(64,128,2,2,start_with_relu=False,grow_first=True)\n",
        "        self.block2=Block(128,256,2,2,start_with_relu=True,grow_first=True)\n",
        "        self.block3=Block(256,728,2,2,start_with_relu=True,grow_first=True)\n",
        "\n",
        "        self.block4=Block(728,728,3,1,start_with_relu=True,grow_first=True)\n",
        "        self.block5=Block(728,728,3,1,start_with_relu=True,grow_first=True)\n",
        "        self.block6=Block(728,728,3,1,start_with_relu=True,grow_first=True)\n",
        "        self.block7=Block(728,728,3,1,start_with_relu=True,grow_first=True)\n",
        "\n",
        "        self.block8=Block(728,728,3,1,start_with_relu=True,grow_first=True)\n",
        "        self.block9=Block(728,728,3,1,start_with_relu=True,grow_first=True)\n",
        "        self.block10=Block(728,728,3,1,start_with_relu=True,grow_first=True)\n",
        "        self.block11=Block(728,728,3,1,start_with_relu=True,grow_first=True)\n",
        "\n",
        "        self.block12=Block(728,1024,2,2,start_with_relu=True,grow_first=False)\n",
        "\n",
        "        self.conv3 = SeparableConv2d(1024,1536,3,1,1)\n",
        "        self.bn3 = nn.BatchNorm2d(1536)\n",
        "\n",
        "        #do relu here\n",
        "        self.conv4 = SeparableConv2d(1536,2048,3,1,1)\n",
        "        self.bn4 = nn.BatchNorm2d(2048)\n",
        "\n",
        "        # Missing 'fc <- Linear(2048, 1000)' and 'dp <- Dropout(p=0.2)' layers\n",
        "\n",
        "        # Missing 'forward' function\n",
        "\n",
        "def return_pytorch04_xception(pretrained=True):\n",
        "    model = Xception(num_classes=1000)\n",
        "    if pretrained:\n",
        "        state_dict = torch.load(\n",
        "            'checkpoints/F3Net/xception-b5690688.pth')\n",
        "        for name, weights in state_dict.items():\n",
        "            if 'pointwise' in name:\n",
        "                state_dict[name] = weights.unsqueeze(-1).unsqueeze(-1)\n",
        "        model.load_state_dict(state_dict, strict=False)\n",
        " \n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aG8cOCphGBHe"
      },
      "outputs": [],
      "source": [
        "# utils\n",
        "def DCT_mat(size):\n",
        "    m = [[ (np.sqrt(1./size) if i == 0 else np.sqrt(2./size)) * np.cos((j + 0.5) * np.pi * i / size) for j in range(size)] for i in range(size)]\n",
        "    return m\n",
        "\n",
        "def generate_filter(start, end, size):\n",
        "    return [[0. if i + j > end or i + j <= start else 1. for j in range(size)] for i in range(size)]\n",
        "\n",
        "def norm_sigma(x):\n",
        "    return 2. * torch.sigmoid(x) - 1.\n",
        "\n",
        "class Filter(nn.Module):\n",
        "    def __init__(self, size, \n",
        "                 band_start, \n",
        "                 band_end, \n",
        "                 use_learnable=True, \n",
        "                 norm=False):\n",
        "        super(Filter, self).__init__()\n",
        "        self.use_learnable = use_learnable\n",
        "\n",
        "        self.base = nn.Parameter(torch.tensor(generate_filter(band_start, band_end, size)), requires_grad=False)\n",
        "        if self.use_learnable:\n",
        "            self.learnable = nn.Parameter(torch.randn(size, size), requires_grad=True)\n",
        "            self.learnable.data.normal_(0., 0.1)\n",
        "            # REPEATING OPERATIONS?\n",
        "            # Todo\n",
        "            # self.learnable = nn.Parameter(torch.rand((size, size)) * 0.2 - 0.1, requires_grad=True)\n",
        "\n",
        "        self.norm = norm\n",
        "        if norm:\n",
        "            self.ft_num = nn.Parameter(torch.sum(torch.tensor(generate_filter(band_start, band_end, size))), requires_grad=False)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.use_learnable:\n",
        "            filt = self.base + norm_sigma(self.learnable)\n",
        "        else:\n",
        "            filt = self.base\n",
        "\n",
        "        if self.norm:\n",
        "            y = x * filt / self.ft_num\n",
        "        else:\n",
        "            y = x * filt\n",
        "        return y\n",
        "\n",
        "class FAD_Head(nn.Module):\n",
        "    def __init__(self, size):\n",
        "        super(FAD_Head, self).__init__()\n",
        "        # init DCT matrix\n",
        "        self._DCT_all = nn.Parameter(torch.tensor(DCT_mat(size)).float(), requires_grad=False)\n",
        "        self._DCT_all_T = nn.Parameter(torch.transpose(torch.tensor(DCT_mat(size)).float(), 0, 1), requires_grad=False)\n",
        "\n",
        "        # define base filters and learnable\n",
        "        # 0 - 1/16 || 1/16 - 1/8 || 1/8 - 1\n",
        "        low_filter = Filter(size, 0, size // 16)\n",
        "        middle_filter = Filter(size, size // 16, size // 8)\n",
        "        high_filter = Filter(size, size // 8, size)\n",
        "        all_filter = Filter(size, 0, size * 2)\n",
        "        # BANDS ARE DIFFERENT\n",
        "\n",
        "        self.filters = nn.ModuleList([low_filter, middle_filter, high_filter, all_filter])\n",
        "\n",
        "    def forward(self, x):\n",
        "        # DCT\n",
        "        x_freq = self._DCT_all @ x @ self._DCT_all_T    # [N, 3, 200, 200]\n",
        "\n",
        "        # 4 kernel\n",
        "        y_list = []\n",
        "        for i in range(4):\n",
        "            x_pass = self.filters[i](x_freq)  # [N, 3, 200, 200]\n",
        "            y = self._DCT_all_T @ x_pass @ self._DCT_all    # [N, 3, 200, 200]\n",
        "            y_list.append(y)\n",
        "        out = torch.cat(y_list, dim=1)    # [N, 12, 200, 200]\n",
        "        return out\n",
        "\n",
        "class LFS_Head(nn.Module):\n",
        "    def __init__(self, size, window_size, M):\n",
        "        super(LFS_Head, self).__init__()\n",
        "\n",
        "        self.window_size = window_size\n",
        "        self._M = M\n",
        "\n",
        "        # init DCT matrix\n",
        "        self._DCT_patch = nn.Parameter(torch.tensor(DCT_mat(window_size)).float(), requires_grad=False)\n",
        "        self._DCT_patch_T = nn.Parameter(torch.transpose(torch.tensor(DCT_mat(window_size)).float(), 0, 1), requires_grad=False)\n",
        "\n",
        "        self.unfold = nn.Unfold(kernel_size=(window_size, window_size), stride=2, padding=4)\n",
        "\n",
        "        # init filters\n",
        "        self.filters = nn.ModuleList([Filter(window_size, window_size * 2. / M * i, window_size * 2. / M * (i+1), norm=True) for i in range(M)])\n",
        "    \n",
        "    def forward(self, x):\n",
        "        # turn RGB into Gray\n",
        "        x_gray = 0.299*x[:,0,:,:] + 0.587*x[:,1,:,:] + 0.114*x[:,2,:,:]\n",
        "        x = x_gray.unsqueeze(1)\n",
        "\n",
        "        # rescale to 0 - 255\n",
        "        x = (x + 1.) * 122.5\n",
        "\n",
        "        # calculate size\n",
        "        N, C, W, H = x.size()\n",
        "        S = self.window_size\n",
        "        size_after = int((W - S + 8)/2) + 1\n",
        "        assert size_after == 100\n",
        "\n",
        "        # sliding window unfold and DCT\n",
        "        x_unfold = self.unfold(x)   # [N, C * S * S, L]   L:block num\n",
        "        L = x_unfold.size()[2]\n",
        "        x_unfold = x_unfold.transpose(1, 2).reshape(N, L, C, S, S)  # [N, L, C, S, S]\n",
        "        x_dct = self._DCT_patch @ x_unfold @ self._DCT_patch_T\n",
        "\n",
        "        # M kernels filtering\n",
        "        y_list = []\n",
        "        for i in range(self._M):\n",
        "            # y = self.filters[i](x_dct)    # [N, L, C, S, S]\n",
        "            # y = torch.abs(y)\n",
        "            # y = torch.sum(y, dim=[2,3,4])   # [N, L]\n",
        "            # y = torch.log10(y + 1e-15)\n",
        "            y = torch.abs(x_dct)\n",
        "            y = torch.log10(y + 1e-15)\n",
        "            y = self.filters[i](y)\n",
        "            y = torch.sum(y, dim=[2,3,4])\n",
        "            y = y.reshape(N, size_after, size_after).unsqueeze(dim=1)   # [N, 1, 100, 100]\n",
        "            y_list.append(y)\n",
        "        out = torch.cat(y_list, dim=1)  # [N, M, 100, 100]\n",
        "        return out\n",
        "\n",
        "class MixBlock(nn.Module):\n",
        "    \n",
        "    def __init__(self, c_in, width, height):\n",
        "        super(MixBlock, self).__init__()\n",
        "        self.FAD_query = nn.Conv2d(c_in, c_in, (1,1))\n",
        "        self.LFS_query = nn.Conv2d(c_in, c_in, (1,1))\n",
        "\n",
        "        self.FAD_key = nn.Conv2d(c_in, c_in, (1,1))\n",
        "        self.LFS_key = nn.Conv2d(c_in, c_in, (1,1))\n",
        "\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "        self.FAD_gamma = nn.Parameter(torch.zeros(1))\n",
        "        self.LFS_gamma = nn.Parameter(torch.zeros(1))\n",
        "\n",
        "        self.FAD_conv = nn.Conv2d(c_in, c_in, (1,1), groups=c_in)\n",
        "        self.FAD_bn = nn.BatchNorm2d(c_in)\n",
        "        self.LFS_conv = nn.Conv2d(c_in, c_in, (1,1), groups=c_in)\n",
        "        self.LFS_bn = nn.BatchNorm2d(c_in)\n",
        "\n",
        "    def forward(self, x_FAD, x_LFS):\n",
        "        B, C, W, H = x_FAD.size()\n",
        "        assert W == H\n",
        "\n",
        "        q_FAD = self.FAD_query(x_FAD).view(-1, W, H)    # [BC, W, H]\n",
        "        q_LFS = self.LFS_query(x_LFS).view(-1, W, H)\n",
        "        M_query = torch.cat([q_FAD, q_LFS], dim=2)  # [BC, W, 2H]\n",
        "\n",
        "        k_FAD = self.FAD_key(x_FAD).view(-1, W, H).transpose(1, 2)  # [BC, H, W]\n",
        "        k_LFS = self.LFS_key(x_LFS).view(-1, W, H).transpose(1, 2)\n",
        "        M_key = torch.cat([k_FAD, k_LFS], dim=1)    # [BC, 2H, W]\n",
        "\n",
        "        energy = torch.bmm(M_query, M_key)  #[BC, W, W]\n",
        "        attention = self.softmax(energy).view(B, C, W, W)\n",
        "\n",
        "        att_LFS = x_LFS * attention * (torch.sigmoid(self.LFS_gamma) * 2.0 - 1.0)\n",
        "        y_FAD = x_FAD + self.FAD_bn(self.FAD_conv(att_LFS))\n",
        "\n",
        "        att_FAD = x_FAD * attention * (torch.sigmoid(self.FAD_gamma) * 2.0 - 1.0)\n",
        "        y_LFS = x_LFS + self.LFS_bn(self.LFS_conv(att_FAD))\n",
        "        return y_FAD, y_LFS\n",
        "\n",
        "class F3Net(nn.Module):\n",
        "    \"\"\"\n",
        "    Implementation is mainly referenced from https://github.com/yyk-wew/F3Net\n",
        "    \"\"\"\n",
        "    def __init__(self, \n",
        "                 num_classes: int=2, \n",
        "                 img_width: int=200, \n",
        "                 img_height: int=200, \n",
        "                 LFS_window_size: int=10, \n",
        "                 LFS_M: int=6) -> None:\n",
        "        super(F3Net, self).__init__()\n",
        "        assert img_width == img_height\n",
        "        self.img_size = img_width\n",
        "        self.num_classes = num_classes\n",
        "        self._LFS_window_size = LFS_window_size\n",
        "        self._LFS_M = LFS_M\n",
        "        \n",
        "        \n",
        "        self.fad_head = FAD_Head(self.img_size)\n",
        "        self.lfs_head = LFS_Head(self.img_size, self._LFS_window_size, self._LFS_M)\n",
        "        \n",
        "        self.fad_excep = self._init_xcep_fad()\n",
        "        self.lfs_excep = self._init_xcep_lfs()\n",
        "        \n",
        "        self.mix_block7 = MixBlock(c_in=728, width=19, height=19) \n",
        "        self.mix_block12 = MixBlock(c_in=1024, width=10, height=10) \n",
        "        self.excep_forwards = ['conv1', 'bn1', 'relu', 'conv2', 'bn2', 'relu', \n",
        "                               'block1', 'block2', 'block3', 'block4', 'block5', 'block6', \n",
        "                               'block7', 'block8', 'block9', 'block10' , 'block11', 'block12',\n",
        "                               'conv3', 'bn3', 'relu', 'conv4', 'bn4']\n",
        "\n",
        "         # classifier\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.fc = nn.Linear(4096, num_classes)\n",
        "        self.dp = nn.Dropout(p=0.2)\n",
        "        \n",
        "    def _init_xcep_fad(self):\n",
        "        fad_excep =  return_pytorch04_xception(True)\n",
        "        conv1_data = fad_excep.conv1.weight.data\n",
        "        # let new conv1 use old param to balance the network\n",
        "        fad_excep.conv1 = nn.Conv2d(12, 32, 3, 2, 0, bias=False)\n",
        "        for i in range(4):\n",
        "            fad_excep.conv1.weight.data[:, i*3:(i+1)*3, :, :] = conv1_data / 4.0\n",
        "        return fad_excep\n",
        "    \n",
        "    def  _init_xcep_lfs(self): \n",
        "        lfs_excep = return_pytorch04_xception(True)\n",
        "        conv1_data = lfs_excep.conv1.weight.data\n",
        "        # let new conv1 use old param to balance the network\n",
        "        lfs_excep.conv1 = nn.Conv2d(self._LFS_M, 32, 3, 1, 1, bias=False)\n",
        "        for i in range(int(self._LFS_M / 3)):\n",
        "            lfs_excep.conv1.weight.data[:, i*3:(i+1)*3, :, :] = conv1_data / float(self._LFS_M / 3.0)\n",
        "        return lfs_excep\n",
        "    \n",
        "    def _features(self, x_fad, x_fls):\n",
        "        for forward_func in self.excep_forwards:\n",
        "            x_fad = getattr(self.fad_excep, forward_func)(x_fad)\n",
        "            x_fls = getattr(self.lfs_excep, forward_func)(x_fls)\n",
        "            if forward_func == 'block7':\n",
        "                x_fad, x_fls = self.mix_block7(x_fad, x_fls)\n",
        "            if forward_func == 'block12':\n",
        "                x_fad, x_fls = self.mix_block12(x_fad, x_fls)\n",
        "        return x_fad, x_fls\n",
        "    \n",
        "    def _norm_feature(self, x):\n",
        "        x = self.relu(x)\n",
        "        x = F.adaptive_avg_pool2d(x, (1,1))\n",
        "        x = x.view(x.size(0), -1)\n",
        "        return x\n",
        "    \n",
        "    def forward(self, x):\n",
        "        fad_input = self.fad_head(x)\n",
        "        lfs_input = self.lfs_head(x)\n",
        "        x_fad, x_fls = self._features(fad_input, lfs_input)\n",
        "        x_fad = self._norm_feature(x_fad)\n",
        "        x_fls = self._norm_feature(x_fls)\n",
        "        x_cat = torch.cat((x_fad, x_fls), dim=1)\n",
        "        x_drop = self.dp(x_cat)\n",
        "        logit = self.fc(x_drop)\n",
        "        return x_cat, logit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yf22QYi8CdwJ"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, mode='valid', recorder=None):\n",
        "    if mode=='valid': dl = val_dl\n",
        "    elif mode=='test': dl = test_dl\n",
        "    else:\n",
        "        print('Unknown mode')\n",
        "        return 0,0,0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        y_true, y_pred = [], []\n",
        "\n",
        "        for i, (data,label) in enumerate(dl):\n",
        "            output = model.forward(data)\n",
        "            y_pred.extend(torch.argmax(output.sigmoid(), dim=-1).flatten().tolist())\n",
        "            y_true.extend(label.flatten().tolist())\n",
        "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
        "    y_true, y_pred = torch.tensor(y_true), torch.tensor(y_pred)\n",
        "\n",
        "    idx_real = np.where(y_true==0)[0]\n",
        "    idx_fake = np.where(y_true==1)[0]\n",
        "\n",
        "    acc = accuracy_score(y_true, y_pred > 0.5)\n",
        "    r_acc = accuracy_score(y_true[idx_real], y_pred[idx_real] > 0.5)\n",
        "    f_acc = accuracy_score(y_true[idx_fake], y_pred[idx_fake] > 0.5)\n",
        "\n",
        "    for i in record_list:\n",
        "        recorder[i].append(metrics[i](y_pred, y_true))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sfxnle_-C2lg"
      },
      "outputs": [],
      "source": [
        "def initModel(mod, gpu_ids):\n",
        "    mod = mod.to(f'cuda:{gpu_ids[0]}')\n",
        "    mod = nn.DataParallel(mod, gpu_ids)\n",
        "    return mod\n",
        "\n",
        "class Trainer(): \n",
        "    def __init__(self, gpu_ids):\n",
        "        self.model = F3Net()\n",
        "        self.model = initModel(self.model, gpu_ids)\n",
        "        self.loss_fn = nn.BCEWithLogitsLoss()\n",
        "        self.optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, self.model.parameters()),\n",
        "                                              lr=0.0002, betas=(0.9, 0.999))\n",
        "        self.total_steps = 0\n",
        "        # self.optimizer = torch.optim.SGD(filter(lambda p: p.requires_grad, self.model.parameters()),\n",
        "        #                                         lr=0.002, momentum=0.9, weight_decay=0)\n",
        "    \n",
        "    def __call__(self, data, label):\n",
        "        stu_fea, stu_cla = self.model(data)\n",
        "\n",
        "        self.loss_cla = self.loss_fn(stu_cla.squeeze(1), label) # classify loss\n",
        "        self.loss = self.loss_cla\n",
        "\n",
        "        self.optimizer.zero_grad()\n",
        "        self.loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        return self.loss\n",
        "    \n",
        "    def forward(self, data):\n",
        "        stu_fea, stu_cla = self.model(data)\n",
        "        return stu_cla\n",
        "\n",
        "    def save(self, path):\n",
        "        torch.save(self.model.state_dict(), path)\n",
        "\n",
        "    def load(self, path):\n",
        "        state_dict = torch.load(path)\n",
        "        self.model.load_state_dict(state_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "22vFCeS2C7cY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4e48ca4-6dfa-491c-a047-b885f039f0af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "loss: 0.5752686262130737 at step: 40\n",
            "loss: 0.6340379118919373 at step: 80\n",
            "loss: 0.731475293636322 at step: 120\n",
            "loss: 0.38900837302207947 at step: 160\n",
            "loss: 0.414130300283432 at step: 200\n",
            "loss: 0.46421730518341064 at step: 240\n",
            "loss: 0.563730001449585 at step: 280\n",
            "loss: 0.338836133480072 at step: 320\n",
            "loss: 0.46689528226852417 at step: 360\n",
            "loss: 0.2763415575027466 at step: 400\n",
            "loss: 0.4023626446723938 at step: 440\n",
            "loss: 0.6912767887115479 at step: 480\n",
            "loss: 0.6350280046463013 at step: 520\n",
            "loss: 0.46378281712532043 at step: 560\n",
            "loss: 0.38623175024986267 at step: 600\n",
            "loss: 0.35194098949432373 at step: 640\n",
            "loss: 0.3239670991897583 at step: 680\n",
            "loss: 0.41498130559921265 at step: 720\n",
            "loss: 0.41055193543434143 at step: 760\n",
            "loss: 0.7813778519630432 at step: 800\n",
            "loss: 0.24339568614959717 at step: 840\n",
            "loss: 0.7975344657897949 at step: 880\n",
            "loss: 0.2987203299999237 at step: 920\n",
            "loss: 0.43118417263031006 at step: 960\n",
            "loss: 0.5094676613807678 at step: 1000\n",
            "loss: 0.8089606165885925 at step: 1040\n",
            "loss: 0.6751075387001038 at step: 1080\n",
            "loss: 0.6239444613456726 at step: 1120\n",
            "loss: 0.35726797580718994 at step: 1160\n",
            "loss: 0.21965883672237396 at step: 1200\n",
            "loss: 0.27667945623397827 at step: 1240\n",
            "loss: 0.3412145972251892 at step: 1280\n",
            "loss: 0.5494534373283386 at step: 1320\n",
            "loss: 0.38454753160476685 at step: 1360\n",
            "loss: 0.4563365578651428 at step: 1400\n",
            "loss: 0.41044700145721436 at step: 1440\n",
            "loss: 0.18643218278884888 at step: 1480\n",
            "loss: 0.478071391582489 at step: 1520\n",
            "loss: 0.4945529103279114 at step: 1560\n",
            "loss: 0.2895709276199341 at step: 1600\n",
            "loss: 0.6528834104537964 at step: 1640\n",
            "loss: 0.40493157505989075 at step: 1680\n",
            "loss: 0.2479511797428131 at step: 1720\n",
            "loss: 0.29518190026283264 at step: 1760\n",
            "loss: 0.36215823888778687 at step: 1800\n",
            "loss: 0.17576488852500916 at step: 1840\n",
            "loss: 0.4154476523399353 at step: 1880\n",
            "loss: 0.39650630950927734 at step: 1920\n",
            "accuracy: 75.0027\n",
            "recall: 75.0027\n",
            "precision: 77.6931\n",
            "f1_score: 73.7571\n",
            "Epoch 2\n",
            "loss: 0.3636534810066223 at step: 1960\n",
            "loss: 0.5195807218551636 at step: 2000\n",
            "loss: 0.7277027368545532 at step: 2040\n",
            "loss: 0.22882778942584991 at step: 2080\n",
            "loss: 0.31250515580177307 at step: 2120\n",
            "loss: 0.49661868810653687 at step: 2160\n",
            "loss: 0.3843626379966736 at step: 2200\n",
            "loss: 0.7030314207077026 at step: 2240\n",
            "loss: 0.1944783627986908 at step: 2280\n",
            "loss: 0.27685320377349854 at step: 2320\n",
            "loss: 0.16336022317409515 at step: 2360\n",
            "loss: 0.2115897238254547 at step: 2400\n",
            "loss: 0.16852539777755737 at step: 2440\n",
            "loss: 0.20033416152000427 at step: 2480\n",
            "loss: 0.423916757106781 at step: 2520\n",
            "loss: 0.28168225288391113 at step: 2560\n",
            "loss: 0.22716113924980164 at step: 2600\n",
            "loss: 0.13130097091197968 at step: 2640\n",
            "loss: 0.24481293559074402 at step: 2680\n",
            "loss: 0.2847208082675934 at step: 2720\n",
            "loss: 0.47836923599243164 at step: 2760\n",
            "loss: 0.10799252986907959 at step: 2800\n",
            "loss: 0.29624709486961365 at step: 2840\n",
            "loss: 0.38423413038253784 at step: 2880\n",
            "loss: 0.1619410663843155 at step: 2920\n",
            "loss: 0.17906081676483154 at step: 2960\n",
            "loss: 0.36055758595466614 at step: 3000\n",
            "loss: 0.1325121372938156 at step: 3040\n",
            "loss: 0.5878971219062805 at step: 3080\n",
            "loss: 0.2669336795806885 at step: 3120\n",
            "loss: 0.5859929323196411 at step: 3160\n",
            "loss: 0.4682788550853729 at step: 3200\n",
            "loss: 0.681215763092041 at step: 3240\n",
            "loss: 0.3112158477306366 at step: 3280\n",
            "loss: 0.4539529085159302 at step: 3320\n",
            "loss: 0.28611481189727783 at step: 3360\n",
            "loss: 0.6178470849990845 at step: 3400\n",
            "loss: 0.6435863971710205 at step: 3440\n",
            "loss: 0.32939431071281433 at step: 3480\n",
            "loss: 0.19974201917648315 at step: 3520\n",
            "loss: 0.21938109397888184 at step: 3560\n",
            "loss: 0.2655331492424011 at step: 3600\n",
            "loss: 0.36743876338005066 at step: 3640\n",
            "loss: 0.17113620042800903 at step: 3680\n",
            "loss: 0.10386615246534348 at step: 3720\n",
            "loss: 0.05274839326739311 at step: 3760\n",
            "loss: 0.607276976108551 at step: 3800\n",
            "loss: 0.13520127534866333 at step: 3840\n",
            "loss: 0.3074554204940796 at step: 3880\n",
            "accuracy: 78.8516\n",
            "recall: 78.8516\n",
            "precision: 81.8948\n",
            "f1_score: 77.6836\n",
            "Epoch 3\n",
            "loss: 0.24161560833454132 at step: 3920\n",
            "loss: 0.739815890789032 at step: 3960\n",
            "loss: 0.9066442251205444 at step: 4000\n",
            "loss: 0.6050857305526733 at step: 4040\n",
            "loss: 0.10891261696815491 at step: 4080\n",
            "loss: 0.3251613676548004 at step: 4120\n",
            "loss: 0.8004795908927917 at step: 4160\n",
            "loss: 0.4765426218509674 at step: 4200\n",
            "loss: 0.25824064016342163 at step: 4240\n",
            "loss: 0.5765019655227661 at step: 4280\n",
            "loss: 0.48113131523132324 at step: 4320\n",
            "loss: 0.3093958795070648 at step: 4360\n",
            "loss: 0.16165250539779663 at step: 4400\n",
            "loss: 0.29685258865356445 at step: 4440\n",
            "loss: 0.10830144584178925 at step: 4480\n",
            "loss: 0.2595449388027191 at step: 4520\n",
            "loss: 0.18780267238616943 at step: 4560\n",
            "loss: 0.5437735319137573 at step: 4600\n",
            "loss: 0.1111166924238205 at step: 4640\n",
            "loss: 0.22610396146774292 at step: 4680\n",
            "loss: 0.10861344635486603 at step: 4720\n",
            "loss: 0.41013363003730774 at step: 4760\n",
            "loss: 0.6186650395393372 at step: 4800\n",
            "loss: 0.25211507081985474 at step: 4840\n",
            "loss: 0.45657238364219666 at step: 4880\n",
            "loss: 0.7748087644577026 at step: 4920\n",
            "loss: 0.2545243799686432 at step: 4960\n",
            "loss: 0.84305340051651 at step: 5000\n",
            "loss: 0.3053963780403137 at step: 5040\n",
            "loss: 0.301663339138031 at step: 5080\n",
            "loss: 0.20232775807380676 at step: 5120\n",
            "loss: 0.3034028708934784 at step: 5160\n",
            "loss: 0.147500142455101 at step: 5200\n",
            "loss: 0.06228642165660858 at step: 5240\n",
            "loss: 0.3074725568294525 at step: 5280\n",
            "loss: 0.2818226218223572 at step: 5320\n",
            "loss: 0.18558308482170105 at step: 5360\n",
            "loss: 0.18356141448020935 at step: 5400\n",
            "loss: 0.23096349835395813 at step: 5440\n",
            "loss: 0.28854119777679443 at step: 5480\n",
            "loss: 0.0605555921792984 at step: 5520\n",
            "loss: 0.5037373900413513 at step: 5560\n",
            "loss: 0.33680230379104614 at step: 5600\n",
            "loss: 0.20655807852745056 at step: 5640\n",
            "loss: 0.3057934641838074 at step: 5680\n",
            "loss: 0.11992619186639786 at step: 5720\n",
            "loss: 0.674461305141449 at step: 5760\n",
            "loss: 0.7502484321594238 at step: 5800\n",
            "loss: 0.5536149740219116 at step: 5840\n",
            "accuracy: 85.4306\n",
            "recall: 85.4306\n",
            "precision: 85.9893\n",
            "f1_score: 85.0257\n",
            "Epoch 4\n",
            "loss: 0.5144169926643372 at step: 5880\n",
            "loss: 0.3028189241886139 at step: 5920\n",
            "loss: 0.34045326709747314 at step: 5960\n",
            "loss: 0.3918978273868561 at step: 6000\n",
            "loss: 0.12183916568756104 at step: 6040\n",
            "loss: 0.17670318484306335 at step: 6080\n",
            "loss: 0.15657462179660797 at step: 6120\n",
            "loss: 0.4247925877571106 at step: 6160\n",
            "loss: 0.23669281601905823 at step: 6200\n",
            "loss: 0.14438572525978088 at step: 6240\n",
            "loss: 0.20797410607337952 at step: 6280\n",
            "loss: 0.4737958312034607 at step: 6320\n",
            "loss: 0.14323773980140686 at step: 6360\n",
            "loss: 0.2869619131088257 at step: 6400\n",
            "loss: 0.1986040472984314 at step: 6440\n",
            "loss: 0.262127548456192 at step: 6480\n",
            "loss: 0.1515425592660904 at step: 6520\n",
            "loss: 0.3567039966583252 at step: 6560\n",
            "loss: 0.47526782751083374 at step: 6600\n",
            "loss: 0.3265266418457031 at step: 6640\n",
            "loss: 0.15841010212898254 at step: 6680\n",
            "loss: 0.6109695434570312 at step: 6720\n",
            "loss: 0.15249694883823395 at step: 6760\n",
            "loss: 0.14895105361938477 at step: 6800\n",
            "loss: 0.3189699649810791 at step: 6840\n",
            "loss: 0.27616915106773376 at step: 6880\n",
            "loss: 0.5807861089706421 at step: 6920\n",
            "loss: 0.057664379477500916 at step: 6960\n",
            "loss: 0.26389080286026 at step: 7000\n",
            "loss: 0.9358920454978943 at step: 7040\n",
            "loss: 0.06617141515016556 at step: 7080\n",
            "loss: 0.37546831369400024 at step: 7120\n",
            "loss: 0.07987077534198761 at step: 7160\n",
            "loss: 0.18738047778606415 at step: 7200\n",
            "loss: 0.21125978231430054 at step: 7240\n",
            "loss: 0.11704614013433456 at step: 7280\n",
            "loss: 0.6051651239395142 at step: 7320\n",
            "loss: 0.08358536660671234 at step: 7360\n",
            "loss: 0.15783825516700745 at step: 7400\n",
            "loss: 0.18176403641700745 at step: 7440\n",
            "loss: 0.506223738193512 at step: 7480\n",
            "loss: 0.3809885084629059 at step: 7520\n",
            "loss: 0.48722952604293823 at step: 7560\n",
            "loss: 0.09197405725717545 at step: 7600\n",
            "loss: 0.041168615221977234 at step: 7640\n",
            "loss: 0.055286161601543427 at step: 7680\n",
            "loss: 0.09257448464632034 at step: 7720\n",
            "loss: 0.22848321497440338 at step: 7760\n",
            "loss: 0.2603498697280884 at step: 7800\n",
            "accuracy: 87.8478\n",
            "recall: 87.8478\n",
            "precision: 87.9031\n",
            "f1_score: 87.8712\n",
            "Epoch 5\n",
            "loss: 0.0724901482462883 at step: 7840\n",
            "loss: 0.33499306440353394 at step: 7880\n",
            "loss: 0.12591791152954102 at step: 7920\n",
            "loss: 0.5277206301689148 at step: 7960\n",
            "loss: 0.43463876843452454 at step: 8000\n",
            "loss: 0.058105435222387314 at step: 8040\n",
            "loss: 0.2686659097671509 at step: 8080\n",
            "loss: 0.2687613368034363 at step: 8120\n",
            "loss: 0.4199933111667633 at step: 8160\n",
            "loss: 0.237853541970253 at step: 8200\n",
            "loss: 0.33043891191482544 at step: 8240\n",
            "loss: 0.42887353897094727 at step: 8280\n",
            "loss: 0.14747615158557892 at step: 8320\n",
            "loss: 0.10894142836332321 at step: 8360\n",
            "loss: 0.25942033529281616 at step: 8400\n",
            "loss: 0.22173063457012177 at step: 8440\n",
            "loss: 0.08605511486530304 at step: 8480\n",
            "loss: 0.1654152274131775 at step: 8520\n",
            "loss: 0.7394635081291199 at step: 8560\n",
            "loss: 0.09909583628177643 at step: 8600\n",
            "loss: 0.27249401807785034 at step: 8640\n",
            "loss: 0.2978675961494446 at step: 8680\n",
            "loss: 0.18093600869178772 at step: 8720\n",
            "loss: 0.3353496491909027 at step: 8760\n",
            "loss: 0.34406834840774536 at step: 8800\n",
            "loss: 0.46258941292762756 at step: 8840\n",
            "loss: 0.16189606487751007 at step: 8880\n",
            "loss: 0.24870571494102478 at step: 8920\n",
            "loss: 0.15257984399795532 at step: 8960\n",
            "loss: 0.30894941091537476 at step: 9000\n",
            "loss: 0.07469905912876129 at step: 9040\n",
            "loss: 0.685888409614563 at step: 9080\n",
            "loss: 2.0817370414733887 at step: 9120\n",
            "loss: 0.21545952558517456 at step: 9160\n",
            "loss: 0.06676552444696426 at step: 9200\n",
            "loss: 0.02693224884569645 at step: 9240\n",
            "loss: 0.039580151438713074 at step: 9280\n",
            "loss: 0.2667759656906128 at step: 9320\n",
            "loss: 0.427234411239624 at step: 9360\n",
            "loss: 0.08615840971469879 at step: 9400\n",
            "loss: 0.1860671192407608 at step: 9440\n",
            "loss: 0.4199466407299042 at step: 9480\n",
            "loss: 0.2030245065689087 at step: 9520\n",
            "loss: 0.11201734840869904 at step: 9560\n",
            "loss: 0.35830792784690857 at step: 9600\n",
            "loss: 0.15368898212909698 at step: 9640\n",
            "loss: 0.20260922610759735 at step: 9680\n",
            "loss: 0.22006091475486755 at step: 9720\n",
            "loss: 0.4289873242378235 at step: 9760\n",
            "accuracy: 86.3790\n",
            "recall: 86.3790\n",
            "precision: 87.6980\n",
            "f1_score: 86.5518\n",
            "Epoch 6\n",
            "loss: 0.07621806114912033 at step: 9800\n",
            "loss: 0.11343421041965485 at step: 9840\n",
            "loss: 0.8079707622528076 at step: 9880\n",
            "loss: 0.1789504736661911 at step: 9920\n",
            "loss: 0.14915946125984192 at step: 9960\n",
            "loss: 0.18098044395446777 at step: 10000\n",
            "loss: 0.056309811770915985 at step: 10040\n",
            "loss: 0.07807695120573044 at step: 10080\n",
            "loss: 0.2542784810066223 at step: 10120\n",
            "loss: 0.04540642350912094 at step: 10160\n",
            "loss: 0.1563219428062439 at step: 10200\n",
            "loss: 0.09286946803331375 at step: 10240\n",
            "loss: 0.053872041404247284 at step: 10280\n",
            "loss: 0.16265897452831268 at step: 10320\n",
            "loss: 0.387738972902298 at step: 10360\n",
            "loss: 0.09643037617206573 at step: 10400\n",
            "loss: 0.04042148217558861 at step: 10440\n",
            "loss: 0.8298848271369934 at step: 10480\n",
            "loss: 0.5203722715377808 at step: 10520\n",
            "loss: 0.03420032188296318 at step: 10560\n",
            "loss: 0.2611038088798523 at step: 10600\n",
            "loss: 0.06391917169094086 at step: 10640\n",
            "loss: 0.19455692172050476 at step: 10680\n",
            "loss: 0.17905476689338684 at step: 10720\n",
            "loss: 0.2837122082710266 at step: 10760\n",
            "loss: 1.3579652309417725 at step: 10800\n",
            "loss: 0.19029542803764343 at step: 10840\n",
            "loss: 0.03682338818907738 at step: 10880\n",
            "loss: 0.07056057453155518 at step: 10920\n",
            "loss: 0.1977854073047638 at step: 10960\n",
            "loss: 0.2394951730966568 at step: 11000\n",
            "loss: 0.6481978297233582 at step: 11040\n",
            "loss: 0.2880789637565613 at step: 11080\n",
            "loss: 0.16188132762908936 at step: 11120\n",
            "loss: 0.05912786349654198 at step: 11160\n",
            "loss: 0.09700711816549301 at step: 11200\n",
            "loss: 0.1050020307302475 at step: 11240\n",
            "loss: 0.06397289037704468 at step: 11280\n",
            "loss: 0.15452460944652557 at step: 11320\n",
            "loss: 0.408001571893692 at step: 11360\n",
            "loss: 0.20368529856204987 at step: 11400\n",
            "loss: 0.5895736813545227 at step: 11440\n",
            "loss: 0.11986318975687027 at step: 11480\n",
            "loss: 0.024599814787507057 at step: 11520\n",
            "loss: 0.1499439775943756 at step: 11560\n",
            "loss: 0.3476789891719818 at step: 11600\n",
            "loss: 0.13890835642814636 at step: 11640\n",
            "loss: 0.2042263150215149 at step: 11680\n",
            "loss: 0.08523299545049667 at step: 11720\n",
            "accuracy: 84.3046\n",
            "recall: 84.3046\n",
            "precision: 84.6774\n",
            "f1_score: 83.9562\n",
            "Epoch 7\n",
            "loss: 0.09573450684547424 at step: 11760\n",
            "loss: 0.42979833483695984 at step: 11800\n",
            "loss: 0.5711709260940552 at step: 11840\n",
            "loss: 0.2585937976837158 at step: 11880\n",
            "loss: 0.22527267038822174 at step: 11920\n",
            "loss: 0.15144917368888855 at step: 11960\n",
            "loss: 0.29858794808387756 at step: 12000\n",
            "loss: 0.12035202980041504 at step: 12040\n",
            "loss: 0.19562652707099915 at step: 12080\n",
            "loss: 0.43463027477264404 at step: 12120\n",
            "loss: 0.18742415308952332 at step: 12160\n",
            "loss: 0.1268855482339859 at step: 12200\n",
            "loss: 0.06223445385694504 at step: 12240\n",
            "loss: 0.29311123490333557 at step: 12280\n",
            "loss: 0.35669946670532227 at step: 12320\n",
            "loss: 0.16010503470897675 at step: 12360\n",
            "loss: 0.3886619210243225 at step: 12400\n",
            "loss: 0.12003397196531296 at step: 12440\n",
            "loss: 0.20423777401447296 at step: 12480\n",
            "loss: 0.10086509585380554 at step: 12520\n",
            "loss: 0.1778954565525055 at step: 12560\n",
            "loss: 0.22243979573249817 at step: 12600\n",
            "loss: 0.15697962045669556 at step: 12640\n",
            "loss: 0.22112828493118286 at step: 12680\n",
            "loss: 0.02784469723701477 at step: 12720\n",
            "loss: 0.09084197133779526 at step: 12760\n",
            "loss: 0.023855194449424744 at step: 12800\n",
            "loss: 0.16236476600170135 at step: 12840\n",
            "loss: 0.05835869163274765 at step: 12880\n",
            "loss: 0.27621302008628845 at step: 12920\n",
            "loss: 0.18096092343330383 at step: 12960\n",
            "loss: 0.28039124608039856 at step: 13000\n",
            "loss: 0.30470144748687744 at step: 13040\n",
            "loss: 0.07960044592618942 at step: 13080\n",
            "loss: 0.10662170499563217 at step: 13120\n",
            "loss: 0.17574156820774078 at step: 13160\n",
            "loss: 0.1287844181060791 at step: 13200\n",
            "loss: 0.04178481176495552 at step: 13240\n",
            "loss: 0.15579350292682648 at step: 13280\n",
            "loss: 0.05403163284063339 at step: 13320\n",
            "loss: 0.3067026734352112 at step: 13360\n",
            "loss: 0.41963857412338257 at step: 13400\n",
            "loss: 0.2724990248680115 at step: 13440\n",
            "loss: 0.16789758205413818 at step: 13480\n",
            "loss: 0.05184391885995865 at step: 13520\n",
            "loss: 0.30466362833976746 at step: 13560\n",
            "loss: 0.19748198986053467 at step: 13600\n",
            "loss: 0.588513195514679 at step: 13640\n",
            "loss: 0.1498585045337677 at step: 13680\n",
            "accuracy: 92.3159\n",
            "recall: 92.3159\n",
            "precision: 92.3181\n",
            "f1_score: 92.3170\n",
            "Epoch 8\n",
            "loss: 0.04975339025259018 at step: 13720\n",
            "loss: 0.36408156156539917 at step: 13760\n",
            "loss: 0.575051486492157 at step: 13800\n",
            "loss: 0.04942822456359863 at step: 13840\n",
            "loss: 0.07713232934474945 at step: 13880\n",
            "loss: 0.35921841859817505 at step: 13920\n",
            "loss: 0.04784390330314636 at step: 13960\n",
            "loss: 0.11100523173809052 at step: 14000\n",
            "loss: 0.11382046341896057 at step: 14040\n",
            "loss: 0.37125611305236816 at step: 14080\n",
            "loss: 0.1762925684452057 at step: 14120\n",
            "loss: 0.25668054819107056 at step: 14160\n",
            "loss: 0.350325345993042 at step: 14200\n",
            "loss: 0.23756739497184753 at step: 14240\n",
            "loss: 0.021353449672460556 at step: 14280\n",
            "loss: 0.031538888812065125 at step: 14320\n",
            "loss: 0.3542163670063019 at step: 14360\n",
            "loss: 0.03408617526292801 at step: 14400\n",
            "loss: 0.17346754670143127 at step: 14440\n",
            "loss: 0.12415395677089691 at step: 14480\n",
            "loss: 0.08013089001178741 at step: 14520\n",
            "loss: 0.25681036710739136 at step: 14560\n",
            "loss: 0.09155839681625366 at step: 14600\n",
            "loss: 0.46359050273895264 at step: 14640\n",
            "loss: 0.18829810619354248 at step: 14680\n",
            "loss: 0.2385731190443039 at step: 14720\n",
            "loss: 0.1288515031337738 at step: 14760\n",
            "loss: 0.19276627898216248 at step: 14800\n",
            "loss: 0.6855620741844177 at step: 14840\n",
            "loss: 0.2029203176498413 at step: 14880\n",
            "loss: 0.12198324501514435 at step: 14920\n",
            "loss: 0.06408369541168213 at step: 14960\n",
            "loss: 0.20121750235557556 at step: 15000\n",
            "loss: 0.12401437759399414 at step: 15040\n",
            "loss: 0.04169989377260208 at step: 15080\n",
            "loss: 0.47260338068008423 at step: 15120\n",
            "loss: 0.37971746921539307 at step: 15160\n",
            "loss: 0.2437962293624878 at step: 15200\n",
            "loss: 0.3848220407962799 at step: 15240\n",
            "loss: 0.12535524368286133 at step: 15280\n",
            "loss: 0.07846858352422714 at step: 15320\n",
            "loss: 0.12290748208761215 at step: 15360\n",
            "loss: 0.12038154155015945 at step: 15400\n",
            "loss: 0.11160299181938171 at step: 15440\n",
            "loss: 0.23600555956363678 at step: 15480\n",
            "loss: 0.07265730202198029 at step: 15520\n",
            "loss: 0.11279197037220001 at step: 15560\n",
            "loss: 0.13118866086006165 at step: 15600\n",
            "loss: 0.23323126137256622 at step: 15640\n",
            "accuracy: 90.9319\n",
            "recall: 90.9319\n",
            "precision: 91.1674\n",
            "f1_score: 90.6406\n",
            "Epoch 9\n",
            "loss: 0.4624585509300232 at step: 15680\n",
            "loss: 0.04089155048131943 at step: 15720\n",
            "loss: 0.09173361212015152 at step: 15760\n",
            "loss: 0.05906302109360695 at step: 15800\n",
            "loss: 0.4029058516025543 at step: 15840\n",
            "loss: 0.2961616516113281 at step: 15880\n",
            "loss: 0.05960065871477127 at step: 15920\n",
            "loss: 0.08113086223602295 at step: 15960\n",
            "loss: 0.38535022735595703 at step: 16000\n",
            "loss: 0.08389401435852051 at step: 16040\n",
            "loss: 0.2922525107860565 at step: 16080\n",
            "loss: 0.08188506215810776 at step: 16120\n",
            "loss: 0.21517786383628845 at step: 16160\n",
            "loss: 0.8030733466148376 at step: 16200\n",
            "loss: 0.18419137597084045 at step: 16240\n",
            "loss: 0.0280703566968441 at step: 16280\n",
            "loss: 0.14477548003196716 at step: 16320\n",
            "loss: 0.19905626773834229 at step: 16360\n",
            "loss: 0.7994834780693054 at step: 16400\n",
            "loss: 0.7552041411399841 at step: 16440\n",
            "loss: 0.051517970860004425 at step: 16480\n",
            "loss: 0.25217705965042114 at step: 16520\n",
            "loss: 0.07720562070608139 at step: 16560\n",
            "loss: 0.54352867603302 at step: 16600\n",
            "loss: 0.126613587141037 at step: 16640\n",
            "loss: 0.10969117283821106 at step: 16680\n",
            "loss: 0.11302606761455536 at step: 16720\n",
            "loss: 0.2548616826534271 at step: 16760\n",
            "loss: 0.018880389630794525 at step: 16800\n",
            "loss: 0.1569242775440216 at step: 16840\n",
            "loss: 0.21516823768615723 at step: 16880\n",
            "loss: 0.1779497116804123 at step: 16920\n",
            "loss: 0.30926188826560974 at step: 16960\n",
            "loss: 0.03498290106654167 at step: 17000\n",
            "loss: 1.118564248085022 at step: 17040\n",
            "loss: 0.17223908007144928 at step: 17080\n",
            "loss: 0.23932567238807678 at step: 17120\n",
            "loss: 0.020653173327445984 at step: 17160\n",
            "loss: 0.44201844930648804 at step: 17200\n",
            "loss: 0.7524012327194214 at step: 17240\n",
            "loss: 0.35438960790634155 at step: 17280\n",
            "loss: 0.3615405857563019 at step: 17320\n",
            "loss: 0.66169273853302 at step: 17360\n",
            "loss: 0.07513166964054108 at step: 17400\n",
            "loss: 0.26564210653305054 at step: 17440\n",
            "loss: 0.27201220393180847 at step: 17480\n",
            "loss: 0.13520190119743347 at step: 17520\n",
            "loss: 0.10658979415893555 at step: 17560\n",
            "loss: 0.26497453451156616 at step: 17600\n",
            "accuracy: 92.9427\n",
            "recall: 92.9427\n",
            "precision: 92.8841\n",
            "f1_score: 92.9052\n",
            "Epoch 10\n",
            "loss: 0.12455740571022034 at step: 17640\n",
            "loss: 0.44738173484802246 at step: 17680\n",
            "loss: 0.4493217468261719 at step: 17720\n",
            "loss: 0.3434259295463562 at step: 17760\n",
            "loss: 0.3024410903453827 at step: 17800\n",
            "loss: 0.07803871482610703 at step: 17840\n",
            "loss: 0.2877380847930908 at step: 17880\n",
            "loss: 0.16651660203933716 at step: 17920\n",
            "loss: 0.4591797888278961 at step: 17960\n",
            "loss: 0.37503528594970703 at step: 18000\n",
            "loss: 0.7739955186843872 at step: 18040\n",
            "loss: 0.1659826636314392 at step: 18080\n",
            "loss: 0.1533385068178177 at step: 18120\n",
            "loss: 0.2562708556652069 at step: 18160\n",
            "loss: 0.1660708487033844 at step: 18200\n",
            "loss: 0.12154147028923035 at step: 18240\n",
            "loss: 0.1175059825181961 at step: 18280\n",
            "loss: 0.045667409896850586 at step: 18320\n",
            "loss: 0.7281088829040527 at step: 18360\n",
            "loss: 0.04013198986649513 at step: 18400\n",
            "loss: 0.15649424493312836 at step: 18440\n",
            "loss: 0.07642944902181625 at step: 18480\n",
            "loss: 0.017539359629154205 at step: 18520\n",
            "loss: 0.107909195125103 at step: 18560\n",
            "loss: 0.5315707921981812 at step: 18600\n",
            "loss: 0.21097880601882935 at step: 18640\n",
            "loss: 0.060753144323825836 at step: 18680\n",
            "loss: 0.04128062725067139 at step: 18720\n",
            "loss: 0.06664593517780304 at step: 18760\n",
            "loss: 0.20770753920078278 at step: 18800\n",
            "loss: 0.07383022457361221 at step: 18840\n",
            "loss: 0.4817218482494354 at step: 18880\n",
            "loss: 0.6082139015197754 at step: 18920\n",
            "loss: 0.046290215104818344 at step: 18960\n",
            "loss: 0.01622752845287323 at step: 19000\n",
            "loss: 0.23023131489753723 at step: 19040\n",
            "loss: 0.29731327295303345 at step: 19080\n",
            "loss: 0.04038821533322334 at step: 19120\n",
            "loss: 0.3043557107448578 at step: 19160\n",
            "loss: 0.06707420200109482 at step: 19200\n",
            "loss: 0.1527886986732483 at step: 19240\n",
            "loss: 0.3959784209728241 at step: 19280\n",
            "loss: 0.17228883504867554 at step: 19320\n",
            "loss: 0.17667706310749054 at step: 19360\n",
            "loss: 0.07024849951267242 at step: 19400\n",
            "loss: 0.15245258808135986 at step: 19440\n",
            "loss: 0.34906697273254395 at step: 19480\n",
            "loss: 0.18291312456130981 at step: 19520\n",
            "loss: 0.20865383744239807 at step: 19560\n",
            "accuracy: 91.3521\n",
            "recall: 91.3521\n",
            "precision: 91.7153\n",
            "f1_score: 91.4552\n",
            "Testing\n",
            "accuracy: 92.3823\n",
            "recall: 92.3823\n",
            "precision: 92.2996\n",
            "f1_score: 92.3142\n"
          ]
        }
      ],
      "source": [
        "gpu_ids = [*range(osenvs)]\n",
        "epochs = 10\n",
        "loss_freq = 40\n",
        "val_freq = 1\n",
        "device = torch.device('cuda:{}'.format(gpu_ids[0])) if gpu_ids else torch.device('cpu')\n",
        "metrics = {\n",
        "    'accuracy': Accuracy(num_classes=2, average='macro'),\n",
        "    'recall': Recall(num_classes=2, average='macro'),\n",
        "    'precision': Precision(num_classes=2, average='macro'),\n",
        "    'f1_score': F1Score(num_classes=2, average='macro')\n",
        "}\n",
        "\n",
        "params = {\n",
        "    \"Model-Name\": 'Thinking in Frequency',\n",
        "    'Dataset-Name': 'Combined datasetv1',\n",
        "    \"backbone_model\": \"xception\",\n",
        "    \"output_type\": \"none\",\n",
        "    \"batch_size\": batch_size,\n",
        "    \"learning_rate\": 0.0002, \n",
        "    \"optimizer\": \"Adam\",\n",
        "    \"use_reg\": False\n",
        "}\n",
        "run[\"parameters\"] = params\n",
        "\n",
        "if __name__ == '__main__':   \n",
        "    # train\n",
        "    model = Trainer(gpu_ids)\n",
        "    best_acc = 0\n",
        "    best_model_wts = copy.deepcopy(model.model.state_dict())\n",
        "\n",
        "    record_list = list(metrics.keys())\n",
        "    \n",
        "    for epoch in range(epochs):   \n",
        "        recorder = {metric: [] for metric in record_list}    \n",
        "        # logger.debug(f'No {epoch}')\n",
        "        print(f'Epoch {epoch+1}')\n",
        "        for i,(data,label) in enumerate(train_dl):\n",
        "            model.total_steps += 1\n",
        "\n",
        "            label = torch.nn.functional.one_hot(label, num_classes=2).float()\n",
        "            data, label = data.to(device), label.to(device)\n",
        "\n",
        "            loss = model(data,label)\n",
        "\n",
        "            if model.total_steps % loss_freq == 0:\n",
        "                print(f'loss: {loss} at step: {model.total_steps}')\n",
        "\n",
        "        model.model.eval()\n",
        "        evaluate(model, mode='valid', recorder=recorder)\n",
        "        for i in record_list:\n",
        "            print(f'{i}: {100*sum(recorder[i])/len(recorder[i]):.4f}')\n",
        "        for i in record_list:\n",
        "            run[f'val/{i}'].log(sum(recorder[i])/len(recorder[i]))\n",
        "        acc = sum(recorder['accuracy'])/len(recorder['accuracy'])\n",
        "        if acc > best_acc:\n",
        "            best_acc = acc\n",
        "            best_model_wts = copy.deepcopy(model.model.state_dict())\n",
        "        model.model.train()\n",
        "\n",
        "    model.model.load_state_dict(best_model_wts)\n",
        "\n",
        "    model.model.eval()\n",
        "    print('Testing')\n",
        "    recorder = {metric: [] for metric in record_list}\n",
        "    evaluate(model, mode='test', recorder=recorder)\n",
        "    for i in record_list:\n",
        "        print(f'{i}: {100*sum(recorder[i])/len(recorder[i]):.4f}')\n",
        "    for i in record_list:\n",
        "        run[f'eval/{i}'].log(sum(recorder[i])/len(recorder[i]))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "exp_num = 'VIP-32'\n",
        "model.save(f'/content/checkpoints/F3Net/{exp_num}.pth')\n",
        "run.stop()"
      ],
      "metadata": {
        "id": "lyqVBM3F11m2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df170bf7-2ed5-4c1a-d4bc-f677087e296f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shutting down background jobs, please wait a moment...\n",
            "Done!\n",
            "Waiting for the remaining 1 operations to synchronize with Neptune. Do not kill this process.\n",
            "All 1 operations synced, thanks for waiting!\n",
            "Explore the metadata in the Neptune app:\n",
            "https://app.neptune.ai/Botz/VIPCup-logs/e/VIP-32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchsummary import summary\n",
        "summary(model.model, (3, 200, 200))"
      ],
      "metadata": {
        "id": "dk0VXhqwX2aN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70e2e1c6-f499-4ba2-df9f-db86a0e57ca3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Filter-1          [-1, 3, 200, 200]               0\n",
            "            Filter-2          [-1, 3, 200, 200]               0\n",
            "            Filter-3          [-1, 3, 200, 200]               0\n",
            "            Filter-4          [-1, 3, 200, 200]               0\n",
            "          FAD_Head-5         [-1, 12, 200, 200]               0\n",
            "            Unfold-6           [-1, 100, 10000]               0\n",
            "            Filter-7     [-1, 10000, 1, 10, 10]               0\n",
            "            Filter-8     [-1, 10000, 1, 10, 10]               0\n",
            "            Filter-9     [-1, 10000, 1, 10, 10]               0\n",
            "           Filter-10     [-1, 10000, 1, 10, 10]               0\n",
            "           Filter-11     [-1, 10000, 1, 10, 10]               0\n",
            "           Filter-12     [-1, 10000, 1, 10, 10]               0\n",
            "         LFS_Head-13          [-1, 6, 100, 100]               0\n",
            "           Conv2d-14           [-1, 32, 99, 99]           3,456\n",
            "           Conv2d-15         [-1, 32, 100, 100]           1,728\n",
            "      BatchNorm2d-16           [-1, 32, 99, 99]              64\n",
            "      BatchNorm2d-17         [-1, 32, 100, 100]              64\n",
            "             ReLU-18           [-1, 32, 99, 99]               0\n",
            "             ReLU-19         [-1, 32, 100, 100]               0\n",
            "           Conv2d-20           [-1, 64, 97, 97]          18,432\n",
            "           Conv2d-21           [-1, 64, 98, 98]          18,432\n",
            "      BatchNorm2d-22           [-1, 64, 97, 97]             128\n",
            "      BatchNorm2d-23           [-1, 64, 98, 98]             128\n",
            "             ReLU-24           [-1, 64, 97, 97]               0\n",
            "             ReLU-25           [-1, 64, 98, 98]               0\n",
            "           Conv2d-26           [-1, 64, 97, 97]             576\n",
            "           Conv2d-27          [-1, 128, 97, 97]           8,192\n",
            "  SeparableConv2d-28          [-1, 128, 97, 97]               0\n",
            "      BatchNorm2d-29          [-1, 128, 97, 97]             256\n",
            "             ReLU-30          [-1, 128, 97, 97]               0\n",
            "             ReLU-31          [-1, 128, 97, 97]               0\n",
            "           Conv2d-32          [-1, 128, 97, 97]           1,152\n",
            "           Conv2d-33          [-1, 128, 97, 97]          16,384\n",
            "  SeparableConv2d-34          [-1, 128, 97, 97]               0\n",
            "      BatchNorm2d-35          [-1, 128, 97, 97]             256\n",
            "        MaxPool2d-36          [-1, 128, 49, 49]               0\n",
            "           Conv2d-37          [-1, 128, 49, 49]           8,192\n",
            "      BatchNorm2d-38          [-1, 128, 49, 49]             256\n",
            "            Block-39          [-1, 128, 49, 49]               0\n",
            "           Conv2d-40           [-1, 64, 98, 98]             576\n",
            "           Conv2d-41          [-1, 128, 98, 98]           8,192\n",
            "  SeparableConv2d-42          [-1, 128, 98, 98]               0\n",
            "      BatchNorm2d-43          [-1, 128, 98, 98]             256\n",
            "             ReLU-44          [-1, 128, 98, 98]               0\n",
            "             ReLU-45          [-1, 128, 98, 98]               0\n",
            "           Conv2d-46          [-1, 128, 98, 98]           1,152\n",
            "           Conv2d-47          [-1, 128, 98, 98]          16,384\n",
            "  SeparableConv2d-48          [-1, 128, 98, 98]               0\n",
            "      BatchNorm2d-49          [-1, 128, 98, 98]             256\n",
            "        MaxPool2d-50          [-1, 128, 49, 49]               0\n",
            "           Conv2d-51          [-1, 128, 49, 49]           8,192\n",
            "      BatchNorm2d-52          [-1, 128, 49, 49]             256\n",
            "            Block-53          [-1, 128, 49, 49]               0\n",
            "             ReLU-54          [-1, 128, 49, 49]               0\n",
            "           Conv2d-55          [-1, 128, 49, 49]           1,152\n",
            "           Conv2d-56          [-1, 256, 49, 49]          32,768\n",
            "  SeparableConv2d-57          [-1, 256, 49, 49]               0\n",
            "      BatchNorm2d-58          [-1, 256, 49, 49]             512\n",
            "             ReLU-59          [-1, 256, 49, 49]               0\n",
            "             ReLU-60          [-1, 256, 49, 49]               0\n",
            "           Conv2d-61          [-1, 256, 49, 49]           2,304\n",
            "           Conv2d-62          [-1, 256, 49, 49]          65,536\n",
            "  SeparableConv2d-63          [-1, 256, 49, 49]               0\n",
            "      BatchNorm2d-64          [-1, 256, 49, 49]             512\n",
            "        MaxPool2d-65          [-1, 256, 25, 25]               0\n",
            "           Conv2d-66          [-1, 256, 25, 25]          32,768\n",
            "      BatchNorm2d-67          [-1, 256, 25, 25]             512\n",
            "            Block-68          [-1, 256, 25, 25]               0\n",
            "             ReLU-69          [-1, 128, 49, 49]               0\n",
            "           Conv2d-70          [-1, 128, 49, 49]           1,152\n",
            "           Conv2d-71          [-1, 256, 49, 49]          32,768\n",
            "  SeparableConv2d-72          [-1, 256, 49, 49]               0\n",
            "      BatchNorm2d-73          [-1, 256, 49, 49]             512\n",
            "             ReLU-74          [-1, 256, 49, 49]               0\n",
            "             ReLU-75          [-1, 256, 49, 49]               0\n",
            "           Conv2d-76          [-1, 256, 49, 49]           2,304\n",
            "           Conv2d-77          [-1, 256, 49, 49]          65,536\n",
            "  SeparableConv2d-78          [-1, 256, 49, 49]               0\n",
            "      BatchNorm2d-79          [-1, 256, 49, 49]             512\n",
            "        MaxPool2d-80          [-1, 256, 25, 25]               0\n",
            "           Conv2d-81          [-1, 256, 25, 25]          32,768\n",
            "      BatchNorm2d-82          [-1, 256, 25, 25]             512\n",
            "            Block-83          [-1, 256, 25, 25]               0\n",
            "             ReLU-84          [-1, 256, 25, 25]               0\n",
            "           Conv2d-85          [-1, 256, 25, 25]           2,304\n",
            "           Conv2d-86          [-1, 728, 25, 25]         186,368\n",
            "  SeparableConv2d-87          [-1, 728, 25, 25]               0\n",
            "      BatchNorm2d-88          [-1, 728, 25, 25]           1,456\n",
            "             ReLU-89          [-1, 728, 25, 25]               0\n",
            "             ReLU-90          [-1, 728, 25, 25]               0\n",
            "           Conv2d-91          [-1, 728, 25, 25]           6,552\n",
            "           Conv2d-92          [-1, 728, 25, 25]         529,984\n",
            "  SeparableConv2d-93          [-1, 728, 25, 25]               0\n",
            "      BatchNorm2d-94          [-1, 728, 25, 25]           1,456\n",
            "        MaxPool2d-95          [-1, 728, 13, 13]               0\n",
            "           Conv2d-96          [-1, 728, 13, 13]         186,368\n",
            "      BatchNorm2d-97          [-1, 728, 13, 13]           1,456\n",
            "            Block-98          [-1, 728, 13, 13]               0\n",
            "             ReLU-99          [-1, 256, 25, 25]               0\n",
            "          Conv2d-100          [-1, 256, 25, 25]           2,304\n",
            "          Conv2d-101          [-1, 728, 25, 25]         186,368\n",
            " SeparableConv2d-102          [-1, 728, 25, 25]               0\n",
            "     BatchNorm2d-103          [-1, 728, 25, 25]           1,456\n",
            "            ReLU-104          [-1, 728, 25, 25]               0\n",
            "            ReLU-105          [-1, 728, 25, 25]               0\n",
            "          Conv2d-106          [-1, 728, 25, 25]           6,552\n",
            "          Conv2d-107          [-1, 728, 25, 25]         529,984\n",
            " SeparableConv2d-108          [-1, 728, 25, 25]               0\n",
            "     BatchNorm2d-109          [-1, 728, 25, 25]           1,456\n",
            "       MaxPool2d-110          [-1, 728, 13, 13]               0\n",
            "          Conv2d-111          [-1, 728, 13, 13]         186,368\n",
            "     BatchNorm2d-112          [-1, 728, 13, 13]           1,456\n",
            "           Block-113          [-1, 728, 13, 13]               0\n",
            "            ReLU-114          [-1, 728, 13, 13]               0\n",
            "          Conv2d-115          [-1, 728, 13, 13]           6,552\n",
            "          Conv2d-116          [-1, 728, 13, 13]         529,984\n",
            " SeparableConv2d-117          [-1, 728, 13, 13]               0\n",
            "     BatchNorm2d-118          [-1, 728, 13, 13]           1,456\n",
            "            ReLU-119          [-1, 728, 13, 13]               0\n",
            "            ReLU-120          [-1, 728, 13, 13]               0\n",
            "          Conv2d-121          [-1, 728, 13, 13]           6,552\n",
            "          Conv2d-122          [-1, 728, 13, 13]         529,984\n",
            " SeparableConv2d-123          [-1, 728, 13, 13]               0\n",
            "     BatchNorm2d-124          [-1, 728, 13, 13]           1,456\n",
            "            ReLU-125          [-1, 728, 13, 13]               0\n",
            "            ReLU-126          [-1, 728, 13, 13]               0\n",
            "          Conv2d-127          [-1, 728, 13, 13]           6,552\n",
            "          Conv2d-128          [-1, 728, 13, 13]         529,984\n",
            " SeparableConv2d-129          [-1, 728, 13, 13]               0\n",
            "     BatchNorm2d-130          [-1, 728, 13, 13]           1,456\n",
            "           Block-131          [-1, 728, 13, 13]               0\n",
            "            ReLU-132          [-1, 728, 13, 13]               0\n",
            "          Conv2d-133          [-1, 728, 13, 13]           6,552\n",
            "          Conv2d-134          [-1, 728, 13, 13]         529,984\n",
            " SeparableConv2d-135          [-1, 728, 13, 13]               0\n",
            "     BatchNorm2d-136          [-1, 728, 13, 13]           1,456\n",
            "            ReLU-137          [-1, 728, 13, 13]               0\n",
            "            ReLU-138          [-1, 728, 13, 13]               0\n",
            "          Conv2d-139          [-1, 728, 13, 13]           6,552\n",
            "          Conv2d-140          [-1, 728, 13, 13]         529,984\n",
            " SeparableConv2d-141          [-1, 728, 13, 13]               0\n",
            "     BatchNorm2d-142          [-1, 728, 13, 13]           1,456\n",
            "            ReLU-143          [-1, 728, 13, 13]               0\n",
            "            ReLU-144          [-1, 728, 13, 13]               0\n",
            "          Conv2d-145          [-1, 728, 13, 13]           6,552\n",
            "          Conv2d-146          [-1, 728, 13, 13]         529,984\n",
            " SeparableConv2d-147          [-1, 728, 13, 13]               0\n",
            "     BatchNorm2d-148          [-1, 728, 13, 13]           1,456\n",
            "           Block-149          [-1, 728, 13, 13]               0\n",
            "            ReLU-150          [-1, 728, 13, 13]               0\n",
            "          Conv2d-151          [-1, 728, 13, 13]           6,552\n",
            "          Conv2d-152          [-1, 728, 13, 13]         529,984\n",
            " SeparableConv2d-153          [-1, 728, 13, 13]               0\n",
            "     BatchNorm2d-154          [-1, 728, 13, 13]           1,456\n",
            "            ReLU-155          [-1, 728, 13, 13]               0\n",
            "            ReLU-156          [-1, 728, 13, 13]               0\n",
            "          Conv2d-157          [-1, 728, 13, 13]           6,552\n",
            "          Conv2d-158          [-1, 728, 13, 13]         529,984\n",
            " SeparableConv2d-159          [-1, 728, 13, 13]               0\n",
            "     BatchNorm2d-160          [-1, 728, 13, 13]           1,456\n",
            "            ReLU-161          [-1, 728, 13, 13]               0\n",
            "            ReLU-162          [-1, 728, 13, 13]               0\n",
            "          Conv2d-163          [-1, 728, 13, 13]           6,552\n",
            "          Conv2d-164          [-1, 728, 13, 13]         529,984\n",
            " SeparableConv2d-165          [-1, 728, 13, 13]               0\n",
            "     BatchNorm2d-166          [-1, 728, 13, 13]           1,456\n",
            "           Block-167          [-1, 728, 13, 13]               0\n",
            "            ReLU-168          [-1, 728, 13, 13]               0\n",
            "          Conv2d-169          [-1, 728, 13, 13]           6,552\n",
            "          Conv2d-170          [-1, 728, 13, 13]         529,984\n",
            " SeparableConv2d-171          [-1, 728, 13, 13]               0\n",
            "     BatchNorm2d-172          [-1, 728, 13, 13]           1,456\n",
            "            ReLU-173          [-1, 728, 13, 13]               0\n",
            "            ReLU-174          [-1, 728, 13, 13]               0\n",
            "          Conv2d-175          [-1, 728, 13, 13]           6,552\n",
            "          Conv2d-176          [-1, 728, 13, 13]         529,984\n",
            " SeparableConv2d-177          [-1, 728, 13, 13]               0\n",
            "     BatchNorm2d-178          [-1, 728, 13, 13]           1,456\n",
            "            ReLU-179          [-1, 728, 13, 13]               0\n",
            "            ReLU-180          [-1, 728, 13, 13]               0\n",
            "          Conv2d-181          [-1, 728, 13, 13]           6,552\n",
            "          Conv2d-182          [-1, 728, 13, 13]         529,984\n",
            " SeparableConv2d-183          [-1, 728, 13, 13]               0\n",
            "     BatchNorm2d-184          [-1, 728, 13, 13]           1,456\n",
            "           Block-185          [-1, 728, 13, 13]               0\n",
            "            ReLU-186          [-1, 728, 13, 13]               0\n",
            "          Conv2d-187          [-1, 728, 13, 13]           6,552\n",
            "          Conv2d-188          [-1, 728, 13, 13]         529,984\n",
            " SeparableConv2d-189          [-1, 728, 13, 13]               0\n",
            "     BatchNorm2d-190          [-1, 728, 13, 13]           1,456\n",
            "            ReLU-191          [-1, 728, 13, 13]               0\n",
            "            ReLU-192          [-1, 728, 13, 13]               0\n",
            "          Conv2d-193          [-1, 728, 13, 13]           6,552\n",
            "          Conv2d-194          [-1, 728, 13, 13]         529,984\n",
            " SeparableConv2d-195          [-1, 728, 13, 13]               0\n",
            "     BatchNorm2d-196          [-1, 728, 13, 13]           1,456\n",
            "            ReLU-197          [-1, 728, 13, 13]               0\n",
            "            ReLU-198          [-1, 728, 13, 13]               0\n",
            "          Conv2d-199          [-1, 728, 13, 13]           6,552\n",
            "          Conv2d-200          [-1, 728, 13, 13]         529,984\n",
            " SeparableConv2d-201          [-1, 728, 13, 13]               0\n",
            "     BatchNorm2d-202          [-1, 728, 13, 13]           1,456\n",
            "           Block-203          [-1, 728, 13, 13]               0\n",
            "            ReLU-204          [-1, 728, 13, 13]               0\n",
            "          Conv2d-205          [-1, 728, 13, 13]           6,552\n",
            "          Conv2d-206          [-1, 728, 13, 13]         529,984\n",
            " SeparableConv2d-207          [-1, 728, 13, 13]               0\n",
            "     BatchNorm2d-208          [-1, 728, 13, 13]           1,456\n",
            "            ReLU-209          [-1, 728, 13, 13]               0\n",
            "            ReLU-210          [-1, 728, 13, 13]               0\n",
            "          Conv2d-211          [-1, 728, 13, 13]           6,552\n",
            "          Conv2d-212          [-1, 728, 13, 13]         529,984\n",
            " SeparableConv2d-213          [-1, 728, 13, 13]               0\n",
            "     BatchNorm2d-214          [-1, 728, 13, 13]           1,456\n",
            "            ReLU-215          [-1, 728, 13, 13]               0\n",
            "            ReLU-216          [-1, 728, 13, 13]               0\n",
            "          Conv2d-217          [-1, 728, 13, 13]           6,552\n",
            "          Conv2d-218          [-1, 728, 13, 13]         529,984\n",
            " SeparableConv2d-219          [-1, 728, 13, 13]               0\n",
            "     BatchNorm2d-220          [-1, 728, 13, 13]           1,456\n",
            "           Block-221          [-1, 728, 13, 13]               0\n",
            "            ReLU-222          [-1, 728, 13, 13]               0\n",
            "          Conv2d-223          [-1, 728, 13, 13]           6,552\n",
            "          Conv2d-224          [-1, 728, 13, 13]         529,984\n",
            " SeparableConv2d-225          [-1, 728, 13, 13]               0\n",
            "     BatchNorm2d-226          [-1, 728, 13, 13]           1,456\n",
            "            ReLU-227          [-1, 728, 13, 13]               0\n",
            "            ReLU-228          [-1, 728, 13, 13]               0\n",
            "          Conv2d-229          [-1, 728, 13, 13]           6,552\n",
            "          Conv2d-230          [-1, 728, 13, 13]         529,984\n",
            " SeparableConv2d-231          [-1, 728, 13, 13]               0\n",
            "     BatchNorm2d-232          [-1, 728, 13, 13]           1,456\n",
            "            ReLU-233          [-1, 728, 13, 13]               0\n",
            "            ReLU-234          [-1, 728, 13, 13]               0\n",
            "          Conv2d-235          [-1, 728, 13, 13]           6,552\n",
            "          Conv2d-236          [-1, 728, 13, 13]         529,984\n",
            " SeparableConv2d-237          [-1, 728, 13, 13]               0\n",
            "     BatchNorm2d-238          [-1, 728, 13, 13]           1,456\n",
            "           Block-239          [-1, 728, 13, 13]               0\n",
            "            ReLU-240          [-1, 728, 13, 13]               0\n",
            "          Conv2d-241          [-1, 728, 13, 13]           6,552\n",
            "          Conv2d-242          [-1, 728, 13, 13]         529,984\n",
            " SeparableConv2d-243          [-1, 728, 13, 13]               0\n",
            "     BatchNorm2d-244          [-1, 728, 13, 13]           1,456\n",
            "            ReLU-245          [-1, 728, 13, 13]               0\n",
            "            ReLU-246          [-1, 728, 13, 13]               0\n",
            "          Conv2d-247          [-1, 728, 13, 13]           6,552\n",
            "          Conv2d-248          [-1, 728, 13, 13]         529,984\n",
            " SeparableConv2d-249          [-1, 728, 13, 13]               0\n",
            "     BatchNorm2d-250          [-1, 728, 13, 13]           1,456\n",
            "            ReLU-251          [-1, 728, 13, 13]               0\n",
            "            ReLU-252          [-1, 728, 13, 13]               0\n",
            "          Conv2d-253          [-1, 728, 13, 13]           6,552\n",
            "          Conv2d-254          [-1, 728, 13, 13]         529,984\n",
            " SeparableConv2d-255          [-1, 728, 13, 13]               0\n",
            "     BatchNorm2d-256          [-1, 728, 13, 13]           1,456\n",
            "           Block-257          [-1, 728, 13, 13]               0\n",
            "          Conv2d-258          [-1, 728, 13, 13]         530,712\n",
            "          Conv2d-259          [-1, 728, 13, 13]         530,712\n",
            "          Conv2d-260          [-1, 728, 13, 13]         530,712\n",
            "          Conv2d-261          [-1, 728, 13, 13]         530,712\n",
            "         Softmax-262               [-1, 13, 13]               0\n",
            "          Conv2d-263          [-1, 728, 13, 13]           1,456\n",
            "     BatchNorm2d-264          [-1, 728, 13, 13]           1,456\n",
            "          Conv2d-265          [-1, 728, 13, 13]           1,456\n",
            "     BatchNorm2d-266          [-1, 728, 13, 13]           1,456\n",
            "        MixBlock-267  [[-1, 728, 13, 13], [-1, 728, 13, 13]]               0\n",
            "            ReLU-268          [-1, 728, 13, 13]               0\n",
            "          Conv2d-269          [-1, 728, 13, 13]           6,552\n",
            "          Conv2d-270          [-1, 728, 13, 13]         529,984\n",
            " SeparableConv2d-271          [-1, 728, 13, 13]               0\n",
            "     BatchNorm2d-272          [-1, 728, 13, 13]           1,456\n",
            "            ReLU-273          [-1, 728, 13, 13]               0\n",
            "            ReLU-274          [-1, 728, 13, 13]               0\n",
            "          Conv2d-275          [-1, 728, 13, 13]           6,552\n",
            "          Conv2d-276          [-1, 728, 13, 13]         529,984\n",
            " SeparableConv2d-277          [-1, 728, 13, 13]               0\n",
            "     BatchNorm2d-278          [-1, 728, 13, 13]           1,456\n",
            "            ReLU-279          [-1, 728, 13, 13]               0\n",
            "            ReLU-280          [-1, 728, 13, 13]               0\n",
            "          Conv2d-281          [-1, 728, 13, 13]           6,552\n",
            "          Conv2d-282          [-1, 728, 13, 13]         529,984\n",
            " SeparableConv2d-283          [-1, 728, 13, 13]               0\n",
            "     BatchNorm2d-284          [-1, 728, 13, 13]           1,456\n",
            "           Block-285          [-1, 728, 13, 13]               0\n",
            "            ReLU-286          [-1, 728, 13, 13]               0\n",
            "          Conv2d-287          [-1, 728, 13, 13]           6,552\n",
            "          Conv2d-288          [-1, 728, 13, 13]         529,984\n",
            " SeparableConv2d-289          [-1, 728, 13, 13]               0\n",
            "     BatchNorm2d-290          [-1, 728, 13, 13]           1,456\n",
            "            ReLU-291          [-1, 728, 13, 13]               0\n",
            "            ReLU-292          [-1, 728, 13, 13]               0\n",
            "          Conv2d-293          [-1, 728, 13, 13]           6,552\n",
            "          Conv2d-294          [-1, 728, 13, 13]         529,984\n",
            " SeparableConv2d-295          [-1, 728, 13, 13]               0\n",
            "     BatchNorm2d-296          [-1, 728, 13, 13]           1,456\n",
            "            ReLU-297          [-1, 728, 13, 13]               0\n",
            "            ReLU-298          [-1, 728, 13, 13]               0\n",
            "          Conv2d-299          [-1, 728, 13, 13]           6,552\n",
            "          Conv2d-300          [-1, 728, 13, 13]         529,984\n",
            " SeparableConv2d-301          [-1, 728, 13, 13]               0\n",
            "     BatchNorm2d-302          [-1, 728, 13, 13]           1,456\n",
            "           Block-303          [-1, 728, 13, 13]               0\n",
            "            ReLU-304          [-1, 728, 13, 13]               0\n",
            "          Conv2d-305          [-1, 728, 13, 13]           6,552\n",
            "          Conv2d-306          [-1, 728, 13, 13]         529,984\n",
            " SeparableConv2d-307          [-1, 728, 13, 13]               0\n",
            "     BatchNorm2d-308          [-1, 728, 13, 13]           1,456\n",
            "            ReLU-309          [-1, 728, 13, 13]               0\n",
            "            ReLU-310          [-1, 728, 13, 13]               0\n",
            "          Conv2d-311          [-1, 728, 13, 13]           6,552\n",
            "          Conv2d-312          [-1, 728, 13, 13]         529,984\n",
            " SeparableConv2d-313          [-1, 728, 13, 13]               0\n",
            "     BatchNorm2d-314          [-1, 728, 13, 13]           1,456\n",
            "            ReLU-315          [-1, 728, 13, 13]               0\n",
            "            ReLU-316          [-1, 728, 13, 13]               0\n",
            "          Conv2d-317          [-1, 728, 13, 13]           6,552\n",
            "          Conv2d-318          [-1, 728, 13, 13]         529,984\n",
            " SeparableConv2d-319          [-1, 728, 13, 13]               0\n",
            "     BatchNorm2d-320          [-1, 728, 13, 13]           1,456\n",
            "           Block-321          [-1, 728, 13, 13]               0\n",
            "            ReLU-322          [-1, 728, 13, 13]               0\n",
            "          Conv2d-323          [-1, 728, 13, 13]           6,552\n",
            "          Conv2d-324          [-1, 728, 13, 13]         529,984\n",
            " SeparableConv2d-325          [-1, 728, 13, 13]               0\n",
            "     BatchNorm2d-326          [-1, 728, 13, 13]           1,456\n",
            "            ReLU-327          [-1, 728, 13, 13]               0\n",
            "            ReLU-328          [-1, 728, 13, 13]               0\n",
            "          Conv2d-329          [-1, 728, 13, 13]           6,552\n",
            "          Conv2d-330          [-1, 728, 13, 13]         529,984\n",
            " SeparableConv2d-331          [-1, 728, 13, 13]               0\n",
            "     BatchNorm2d-332          [-1, 728, 13, 13]           1,456\n",
            "            ReLU-333          [-1, 728, 13, 13]               0\n",
            "            ReLU-334          [-1, 728, 13, 13]               0\n",
            "          Conv2d-335          [-1, 728, 13, 13]           6,552\n",
            "          Conv2d-336          [-1, 728, 13, 13]         529,984\n",
            " SeparableConv2d-337          [-1, 728, 13, 13]               0\n",
            "     BatchNorm2d-338          [-1, 728, 13, 13]           1,456\n",
            "           Block-339          [-1, 728, 13, 13]               0\n",
            "            ReLU-340          [-1, 728, 13, 13]               0\n",
            "          Conv2d-341          [-1, 728, 13, 13]           6,552\n",
            "          Conv2d-342          [-1, 728, 13, 13]         529,984\n",
            " SeparableConv2d-343          [-1, 728, 13, 13]               0\n",
            "     BatchNorm2d-344          [-1, 728, 13, 13]           1,456\n",
            "            ReLU-345          [-1, 728, 13, 13]               0\n",
            "            ReLU-346          [-1, 728, 13, 13]               0\n",
            "          Conv2d-347          [-1, 728, 13, 13]           6,552\n",
            "          Conv2d-348          [-1, 728, 13, 13]         529,984\n",
            " SeparableConv2d-349          [-1, 728, 13, 13]               0\n",
            "     BatchNorm2d-350          [-1, 728, 13, 13]           1,456\n",
            "            ReLU-351          [-1, 728, 13, 13]               0\n",
            "            ReLU-352          [-1, 728, 13, 13]               0\n",
            "          Conv2d-353          [-1, 728, 13, 13]           6,552\n",
            "          Conv2d-354          [-1, 728, 13, 13]         529,984\n",
            " SeparableConv2d-355          [-1, 728, 13, 13]               0\n",
            "     BatchNorm2d-356          [-1, 728, 13, 13]           1,456\n",
            "           Block-357          [-1, 728, 13, 13]               0\n",
            "            ReLU-358          [-1, 728, 13, 13]               0\n",
            "          Conv2d-359          [-1, 728, 13, 13]           6,552\n",
            "          Conv2d-360          [-1, 728, 13, 13]         529,984\n",
            " SeparableConv2d-361          [-1, 728, 13, 13]               0\n",
            "     BatchNorm2d-362          [-1, 728, 13, 13]           1,456\n",
            "            ReLU-363          [-1, 728, 13, 13]               0\n",
            "            ReLU-364          [-1, 728, 13, 13]               0\n",
            "          Conv2d-365          [-1, 728, 13, 13]           6,552\n",
            "          Conv2d-366          [-1, 728, 13, 13]         529,984\n",
            " SeparableConv2d-367          [-1, 728, 13, 13]               0\n",
            "     BatchNorm2d-368          [-1, 728, 13, 13]           1,456\n",
            "            ReLU-369          [-1, 728, 13, 13]               0\n",
            "            ReLU-370          [-1, 728, 13, 13]               0\n",
            "          Conv2d-371          [-1, 728, 13, 13]           6,552\n",
            "          Conv2d-372          [-1, 728, 13, 13]         529,984\n",
            " SeparableConv2d-373          [-1, 728, 13, 13]               0\n",
            "     BatchNorm2d-374          [-1, 728, 13, 13]           1,456\n",
            "           Block-375          [-1, 728, 13, 13]               0\n",
            "            ReLU-376          [-1, 728, 13, 13]               0\n",
            "          Conv2d-377          [-1, 728, 13, 13]           6,552\n",
            "          Conv2d-378          [-1, 728, 13, 13]         529,984\n",
            " SeparableConv2d-379          [-1, 728, 13, 13]               0\n",
            "     BatchNorm2d-380          [-1, 728, 13, 13]           1,456\n",
            "            ReLU-381          [-1, 728, 13, 13]               0\n",
            "            ReLU-382          [-1, 728, 13, 13]               0\n",
            "          Conv2d-383          [-1, 728, 13, 13]           6,552\n",
            "          Conv2d-384          [-1, 728, 13, 13]         529,984\n",
            " SeparableConv2d-385          [-1, 728, 13, 13]               0\n",
            "     BatchNorm2d-386          [-1, 728, 13, 13]           1,456\n",
            "            ReLU-387          [-1, 728, 13, 13]               0\n",
            "            ReLU-388          [-1, 728, 13, 13]               0\n",
            "          Conv2d-389          [-1, 728, 13, 13]           6,552\n",
            "          Conv2d-390          [-1, 728, 13, 13]         529,984\n",
            " SeparableConv2d-391          [-1, 728, 13, 13]               0\n",
            "     BatchNorm2d-392          [-1, 728, 13, 13]           1,456\n",
            "           Block-393          [-1, 728, 13, 13]               0\n",
            "            ReLU-394          [-1, 728, 13, 13]               0\n",
            "          Conv2d-395          [-1, 728, 13, 13]           6,552\n",
            "          Conv2d-396          [-1, 728, 13, 13]         529,984\n",
            " SeparableConv2d-397          [-1, 728, 13, 13]               0\n",
            "     BatchNorm2d-398          [-1, 728, 13, 13]           1,456\n",
            "            ReLU-399          [-1, 728, 13, 13]               0\n",
            "            ReLU-400          [-1, 728, 13, 13]               0\n",
            "          Conv2d-401          [-1, 728, 13, 13]           6,552\n",
            "          Conv2d-402          [-1, 728, 13, 13]         529,984\n",
            " SeparableConv2d-403          [-1, 728, 13, 13]               0\n",
            "     BatchNorm2d-404          [-1, 728, 13, 13]           1,456\n",
            "            ReLU-405          [-1, 728, 13, 13]               0\n",
            "            ReLU-406          [-1, 728, 13, 13]               0\n",
            "          Conv2d-407          [-1, 728, 13, 13]           6,552\n",
            "          Conv2d-408          [-1, 728, 13, 13]         529,984\n",
            " SeparableConv2d-409          [-1, 728, 13, 13]               0\n",
            "     BatchNorm2d-410          [-1, 728, 13, 13]           1,456\n",
            "           Block-411          [-1, 728, 13, 13]               0\n",
            "            ReLU-412          [-1, 728, 13, 13]               0\n",
            "          Conv2d-413          [-1, 728, 13, 13]           6,552\n",
            "          Conv2d-414          [-1, 728, 13, 13]         529,984\n",
            " SeparableConv2d-415          [-1, 728, 13, 13]               0\n",
            "     BatchNorm2d-416          [-1, 728, 13, 13]           1,456\n",
            "            ReLU-417          [-1, 728, 13, 13]               0\n",
            "            ReLU-418          [-1, 728, 13, 13]               0\n",
            "          Conv2d-419          [-1, 728, 13, 13]           6,552\n",
            "          Conv2d-420         [-1, 1024, 13, 13]         745,472\n",
            " SeparableConv2d-421         [-1, 1024, 13, 13]               0\n",
            "     BatchNorm2d-422         [-1, 1024, 13, 13]           2,048\n",
            "       MaxPool2d-423           [-1, 1024, 7, 7]               0\n",
            "          Conv2d-424           [-1, 1024, 7, 7]         745,472\n",
            "     BatchNorm2d-425           [-1, 1024, 7, 7]           2,048\n",
            "           Block-426           [-1, 1024, 7, 7]               0\n",
            "            ReLU-427          [-1, 728, 13, 13]               0\n",
            "          Conv2d-428          [-1, 728, 13, 13]           6,552\n",
            "          Conv2d-429          [-1, 728, 13, 13]         529,984\n",
            " SeparableConv2d-430          [-1, 728, 13, 13]               0\n",
            "     BatchNorm2d-431          [-1, 728, 13, 13]           1,456\n",
            "            ReLU-432          [-1, 728, 13, 13]               0\n",
            "            ReLU-433          [-1, 728, 13, 13]               0\n",
            "          Conv2d-434          [-1, 728, 13, 13]           6,552\n",
            "          Conv2d-435         [-1, 1024, 13, 13]         745,472\n",
            " SeparableConv2d-436         [-1, 1024, 13, 13]               0\n",
            "     BatchNorm2d-437         [-1, 1024, 13, 13]           2,048\n",
            "       MaxPool2d-438           [-1, 1024, 7, 7]               0\n",
            "          Conv2d-439           [-1, 1024, 7, 7]         745,472\n",
            "     BatchNorm2d-440           [-1, 1024, 7, 7]           2,048\n",
            "           Block-441           [-1, 1024, 7, 7]               0\n",
            "          Conv2d-442           [-1, 1024, 7, 7]       1,049,600\n",
            "          Conv2d-443           [-1, 1024, 7, 7]       1,049,600\n",
            "          Conv2d-444           [-1, 1024, 7, 7]       1,049,600\n",
            "          Conv2d-445           [-1, 1024, 7, 7]       1,049,600\n",
            "         Softmax-446                 [-1, 7, 7]               0\n",
            "          Conv2d-447           [-1, 1024, 7, 7]           2,048\n",
            "     BatchNorm2d-448           [-1, 1024, 7, 7]           2,048\n",
            "          Conv2d-449           [-1, 1024, 7, 7]           2,048\n",
            "     BatchNorm2d-450           [-1, 1024, 7, 7]           2,048\n",
            "        MixBlock-451  [[-1, 1024, 7, 7], [-1, 1024, 7, 7]]               0\n",
            "          Conv2d-452           [-1, 1024, 7, 7]           9,216\n",
            "          Conv2d-453           [-1, 1536, 7, 7]       1,572,864\n",
            " SeparableConv2d-454           [-1, 1536, 7, 7]               0\n",
            "          Conv2d-455           [-1, 1024, 7, 7]           9,216\n",
            "          Conv2d-456           [-1, 1536, 7, 7]       1,572,864\n",
            " SeparableConv2d-457           [-1, 1536, 7, 7]               0\n",
            "     BatchNorm2d-458           [-1, 1536, 7, 7]           3,072\n",
            "     BatchNorm2d-459           [-1, 1536, 7, 7]           3,072\n",
            "            ReLU-460           [-1, 1536, 7, 7]               0\n",
            "            ReLU-461           [-1, 1536, 7, 7]               0\n",
            "          Conv2d-462           [-1, 1536, 7, 7]          13,824\n",
            "          Conv2d-463           [-1, 2048, 7, 7]       3,145,728\n",
            " SeparableConv2d-464           [-1, 2048, 7, 7]               0\n",
            "          Conv2d-465           [-1, 1536, 7, 7]          13,824\n",
            "          Conv2d-466           [-1, 2048, 7, 7]       3,145,728\n",
            " SeparableConv2d-467           [-1, 2048, 7, 7]               0\n",
            "     BatchNorm2d-468           [-1, 2048, 7, 7]           4,096\n",
            "     BatchNorm2d-469           [-1, 2048, 7, 7]           4,096\n",
            "            ReLU-470           [-1, 2048, 7, 7]               0\n",
            "            ReLU-471           [-1, 2048, 7, 7]               0\n",
            "         Dropout-472                 [-1, 4096]               0\n",
            "          Linear-473                    [-1, 2]           8,194\n",
            "           F3Net-474      [[-1, 4096], [-1, 2]]               0\n",
            "================================================================\n",
            "Total params: 47,960,818\n",
            "Trainable params: 47,960,818\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.46\n",
            "Forward/backward pass size (MB): 133896.54\n",
            "Params size (MB): 182.96\n",
            "Estimated Total Size (MB): 134079.96\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "n4YArIKbq-g6"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "VIP22-Freq.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}